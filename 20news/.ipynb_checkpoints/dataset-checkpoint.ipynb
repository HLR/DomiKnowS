{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "638a9703-0de2-48ca-ae1f-bfd7ab82f8db",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset csv (/home/rajabyfa/.cache/huggingface/datasets/rungalileo___csv/rungalileo--20_Newsgroups_Fixed-edf414ecc72dd622/0.0.0/6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1)\n"
     ]
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.01721930503845215,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "",
       "rate": null,
       "total": 2,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a834a99caa07423a956c792fc65cdc5b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"rungalileo/20_Newsgroups_Fixed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6f336c28-c601-410d-8475-7cbe94056058",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import RobertaTokenizer\n",
    "\n",
    "tokenizer = RobertaTokenizer.from_pretrained(\"roberta-large\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "af6bdfbd-8f94-4e16-809e-bce704240ef4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# comp.graphics\n",
    "# comp.os.ms-windows.misc\n",
    "# comp.sys.ibm.pc.hardware\n",
    "# comp.sys.mac.hardware\n",
    "# comp.windows.x\n",
    "# rec.autos\n",
    "# rec.motorcycles\n",
    "# rec.sport.baseball\n",
    "# rec.sport.hockey\n",
    "# sci.crypt\n",
    "# <sci.electronics\n",
    "# sci.med\n",
    "# sci.space\n",
    "# misc.forsale\n",
    "# talk.politics.misc\n",
    "# talk.politics.guns\n",
    "# talk.politics.mideast\n",
    "# talk.religion.misc\n",
    "# alt.atheism\n",
    "# soc.religion.christian\n",
    "# None\n",
    "\n",
    "import torch\n",
    "\n",
    "\n",
    "def collate_label_set(data):\n",
    "    level1 = [ \n",
    "        \"comp\", \"rec\", \"sci\", \"misc\", \"talk\", \"alt\", \"soc\", \"None\"\n",
    "    ]\n",
    "    level2 = [\n",
    "        \"os\", \"sys\", \"windows\", \"graphics\", \"motorcycles\", \"sport\", \"autos\", \"religion\",\n",
    "        \"electronics\", \"med\", \"space\", \"forsale\", \"politics\", \"crypt\", \"None\"\n",
    "    ]\n",
    "    level2_pass = {\n",
    "        \"soc\", \"alt\" \n",
    "    }\n",
    "    level3_pass = {\n",
    "        \"windows\", \"os\", \"religion\"\n",
    "    }\n",
    "    level3 = [\n",
    "        \"misc\", \"guns\", \"ibm\", \"mac\", \"baseball\", \"hockey\", \"mideast\", \"None\"\n",
    "    ]\n",
    "    for item in data:\n",
    "        if item['text'] is None:\n",
    "            item['text'] = \" \"\n",
    "        try:\n",
    "            label = item['label']\n",
    "            if not label:\n",
    "                label = \"None\"\n",
    "                item['level1'] = level1.index(label)\n",
    "                item['level2'] = level2.index(\"None\")\n",
    "                item['level3'] = level3.index(\"None\")\n",
    "            else:\n",
    "                label = label.split(\".\")\n",
    "                if label[0] not in level1:\n",
    "                    print(label[0])\n",
    "                item['level1'] = level1.index(label[0])\n",
    "                if len(label) > 1:\n",
    "                    if label[0] in level2_pass:\n",
    "                        item['level2'] = level2.index(\"None\")\n",
    "                        item['level3'] = level3.index(\"None\")\n",
    "                    else:\n",
    "                        if label[1] not in level2:\n",
    "                            print(label[1])\n",
    "                            item['level2'] = level2.index(\"None\")\n",
    "                            item['level3'] = level3.index(\"None\")\n",
    "                        else:\n",
    "                            item['level2'] = level2.index(label[1])\n",
    "                            if len(label) > 2:\n",
    "                                if label[1] in level3_pass:\n",
    "                                    item['level3'] = level3.index(\"None\")\n",
    "                                else:\n",
    "                                    if label[2] not in level3:\n",
    "                                        print(label[2])\n",
    "                                        item['level3'] = level3.index(\"None\")\n",
    "                                    else:\n",
    "                                        item['level3'] = level3.index(label[2])\n",
    "                            else:\n",
    "                                item['level3'] = level3.index(\"None\")\n",
    "                else:\n",
    "                    item['level2'] = level2.index(\"None\")\n",
    "                    item['level3'] = level3.index(\"None\")\n",
    "        except:\n",
    "            print(label, item['text'])\n",
    "            raise\n",
    "            \n",
    "    final_data = {\"id\": None, \"text\": None, \"level1\": None, \"level2\": None, \"level3\": None}\n",
    "    final_data['id'] = torch.tensor([item['id'] for item in data])\n",
    "    final_data['text'] = [item['text'] for item in data]\n",
    "    final_data['level1'] = torch.tensor([item['level1'] for item in data])\n",
    "    final_data['level2'] = torch.tensor([item['level2'] for item in data])\n",
    "    final_data['level3'] = torch.tensor([item['level3'] for item in data])\n",
    "    try:\n",
    "        x = tokenizer.batch_encode_plus(final_data['text'], return_tensors=\"pt\", padding=\"longest\", max_length=512, truncation=True)\n",
    "    except:\n",
    "        print(final_data['text'])\n",
    "        raise\n",
    "    for key in x:\n",
    "        final_data[key] = x[key]\n",
    "        \n",
    "    return final_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fe8a6838-5543-4faf-8bc9-16daf7897a44",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "loader = DataLoader(dataset['train'], batch_size=24, pin_memory=True, collate_fn=collate_label_set)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "53629d27-91da-47de-922f-ec32c5064ca8",
   "metadata": {},
   "source": [
    "level1 = { \n",
    "    \"comp\": 0, \"rec\": 0, \"sci\": 0, \"misc\": 0, \"talk\": 0, \"alt\": 0, \"soc\": 0, \"None\": 0\n",
    "}\n",
    "level2_keys = {\n",
    "        \"os\", \"sys\", \"windows\", \"graphics\", \"motorcycles\", \"sport\", \"autos\", \"religion\",\n",
    "        \"electronics\", \"med\", \"space\", \"forsale\", \"politics\", \"religion\", \"crypt\", \"None\"\n",
    "}\n",
    "level2 = {}\n",
    "for _k in level2_keys:\n",
    "    level2[_k] = 0\n",
    "    \n",
    "level3_keys = {\n",
    "    \"misc\", \"guns\", \"ibm\", \"mac\", \"baseball\", \"hockey\", \"mideast\", \"None\"\n",
    "}\n",
    "level3 = {}\n",
    "for _k in level3_keys:\n",
    "    level3[_k] = 0\n",
    "    \n",
    "for x in loader:\n",
    "    # print(x[0].keys())\n",
    "    for y in x:\n",
    "        try:\n",
    "            level1[y['level1']] += 1\n",
    "            level2[y['level2']] += 1\n",
    "            level3[y['level3']] += 1\n",
    "        except:\n",
    "            print(y['label'])\n",
    "            raise\n",
    "    \n",
    "print(level1)\n",
    "print(level2)\n",
    "print(level3)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "999b88fd-ea5e-4e5a-80c4-b061f51b7d91",
   "metadata": {},
   "source": [
    "dataset['train'][0]"
   ]
  },
  {
   "cell_type": "raw",
   "id": "ebbd70ae-e7eb-4c7a-a289-63813ba1054b",
   "metadata": {},
   "source": [
    "next(iter(loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1dcf132a-e17b-4f98-8395-739d8fbe45e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-large were not used when initializing RobertaModel: ['lm_head.decoder.weight', 'lm_head.layer_norm.weight', 'lm_head.dense.bias', 'lm_head.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.bias']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, RobertaModel\n",
    "\n",
    "model_roberta = RobertaModel.from_pretrained(\"roberta-large\")\n",
    "# item = next(iter(loader))\n",
    "# outputs = model_roberta(item['input_ids'], item['attention_mask'])"
   ]
  },
  {
   "cell_type": "raw",
   "id": "4f378861-8b15-412d-8dcb-55a3584ec15f",
   "metadata": {},
   "source": [
    "outputs.pooler_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d242d03f-3b86-49ba-bf57-bfd0677cb953",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class NewsRepModule(nn.Module):\n",
    "    def __init__(self, roberta):\n",
    "        super().__init__()\n",
    "        self.roberta = roberta\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        output = self.roberta(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        logits = output.pooler_output\n",
    "        return logits\n",
    "\n",
    "class Level1Calssification(nn.Module):\n",
    "    def __init__(self,):\n",
    "        super().__init__()\n",
    "        self.classification = nn.Linear(1024, 8)\n",
    "\n",
    "    def forward(self, logits):\n",
    "        _out = self.classification(logits)\n",
    "        return _out\n",
    "    \n",
    "class Level2Calssification(nn.Module):\n",
    "    def __init__(self,):\n",
    "        super().__init__()\n",
    "        self.classification = nn.Linear(1024, 15)\n",
    "\n",
    "    def forward(self, logits):\n",
    "        _out = self.classification(logits)\n",
    "        return _out\n",
    "    \n",
    "class Level3Calssification(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.classification = nn.Linear(1024, 8)\n",
    "\n",
    "    def forward(self, logits):\n",
    "        _out = self.classification(logits)\n",
    "        return _out\n",
    "\n",
    "\n",
    "device = \"cuda:6\"\n",
    "rep_model = NewsRepModule(model_roberta)\n",
    "rep_model = rep_model.to(device)\n",
    "\n",
    "level1_classification = Level1Calssification().to(device)\n",
    "level2_classification = Level2Calssification().to(device)\n",
    "level3_classification = Level3Calssification().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f372dc43-2ccc-47cf-ba83-117774597e8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "params = list(rep_model.parameters()) + list(level1_classification.parameters())\n",
    "params = list(params) + list(level2_classification.parameters()) + list(level3_classification.parameters())\n",
    "optim = torch.optim.AdamW(params, lr=1e-5)\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5558aa69-42a0-4713-a1a9-0504be102207",
   "metadata": {},
   "outputs": [],
   "source": [
    "for item in loader:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a1c4af2c-ed04-4203-9bf0-25856253dd3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 472/472 [12:48<00:00,  1.63s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The loss of epoch 0 is 1115.0531824827194 \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 472/472 [12:49<00:00,  1.63s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The loss of epoch 1 is 550.3676971197128 \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 472/472 [12:49<00:00,  1.63s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The loss of epoch 2 is 384.2522544711828 \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 472/472 [12:49<00:00,  1.63s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The loss of epoch 3 is 275.9331757053733 \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 472/472 [12:51<00:00,  1.63s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The loss of epoch 4 is 202.88637713342905 \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 472/472 [12:49<00:00,  1.63s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The loss of epoch 5 is 140.54915727302432 \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "for epoch in range(6):\n",
    "    rep_model.train()\n",
    "    level1_classification.train()\n",
    "    level2_classification.train()\n",
    "    level3_classification.train()\n",
    "    total_loss = 0\n",
    "    for train_batch in tqdm(loader):\n",
    "        try:\n",
    "            rep = rep_model(train_batch['input_ids'].to(device),\n",
    "                                           train_batch['attention_mask'].to(device))\n",
    "            level1 = level1_classification(rep)\n",
    "            level2 = level2_classification(rep)\n",
    "            level3 = level3_classification(rep)\n",
    "            \n",
    "        except:\n",
    "            print(train_batch['text'])\n",
    "            raise\n",
    "        try:\n",
    "            loss = 0 \n",
    "            loss += criterion(level1, train_batch['level1'].long().to(device))\n",
    "            loss += criterion(level2, train_batch['level2'].long().to(device))\n",
    "            loss += criterion(level3, train_batch['level3'].long().to(device))\n",
    "            total_loss += loss.item()\n",
    "            loss.backward()\n",
    "        except:\n",
    "            print(level1, train_batch['level1'])\n",
    "            print(level2, train_batch['level2'])\n",
    "            print(level3, train_batch['level3'])\n",
    "            raise\n",
    "        optim.step()\n",
    "        optim.zero_grad()\n",
    "    print(f\"The loss of epoch {epoch} is {total_loss} \\n\")\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "04f1640c-ae49-47ee-8402-6c07810f2841",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(rep_model.state_dict(), \"rep_model.pt\")\n",
    "torch.save(level1_classification.state_dict(), \"level1_classification.pt\")\n",
    "torch.save(level2_classification.state_dict(), \"level2_classification.pt\")\n",
    "torch.save(level3_classification.state_dict(), \"level3_classification.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "83366a07-cead-4cda-9314-309782b8bc27",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loader = DataLoader(dataset['test'], batch_size=12, collate_fn=collate_label_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "15018955-6150-4fe9-ae9c-891cd0e98165",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/628 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 24.00 MiB (GPU 6; 47.54 GiB total capacity; 47.03 GiB already allocated; 23.69 MiB free; 47.20 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Input \u001b[0;32mIn [22]\u001b[0m, in \u001b[0;36m<cell line: 9>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m level1_total_actual, level2_total_actual, level3_total_actual \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m train_batch \u001b[38;5;129;01min\u001b[39;00m tqdm(test_loader):\n\u001b[0;32m---> 10\u001b[0m     rep \u001b[38;5;241m=\u001b[39m \u001b[43mrep_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_batch\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43minput_ids\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m                                           \u001b[49m\u001b[43mtrain_batch\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mattention_mask\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     12\u001b[0m     level1 \u001b[38;5;241m=\u001b[39m level1_classification(rep)\n\u001b[1;32m     13\u001b[0m     level2 \u001b[38;5;241m=\u001b[39m level2_classification(rep)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Input \u001b[0;32mIn [13]\u001b[0m, in \u001b[0;36mNewsRepModule.forward\u001b[0;34m(self, input_ids, attention_mask)\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, input_ids, attention_mask):\n\u001b[0;32m----> 9\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mroberta\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     10\u001b[0m     logits \u001b[38;5;241m=\u001b[39m output\u001b[38;5;241m.\u001b[39mpooler_output\n\u001b[1;32m     11\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m logits\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/models/roberta/modeling_roberta.py:845\u001b[0m, in \u001b[0;36mRobertaModel.forward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    838\u001b[0m \u001b[38;5;66;03m# Prepare head mask if needed\u001b[39;00m\n\u001b[1;32m    839\u001b[0m \u001b[38;5;66;03m# 1.0 in head_mask indicate we keep the head\u001b[39;00m\n\u001b[1;32m    840\u001b[0m \u001b[38;5;66;03m# attention_probs has shape bsz x n_heads x N x N\u001b[39;00m\n\u001b[1;32m    841\u001b[0m \u001b[38;5;66;03m# input head_mask has shape [num_heads] or [num_hidden_layers x num_heads]\u001b[39;00m\n\u001b[1;32m    842\u001b[0m \u001b[38;5;66;03m# and head_mask is converted to shape [num_hidden_layers x batch x num_heads x seq_length x seq_length]\u001b[39;00m\n\u001b[1;32m    843\u001b[0m head_mask \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_head_mask(head_mask, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mnum_hidden_layers)\n\u001b[0;32m--> 845\u001b[0m embedding_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membeddings\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    846\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    847\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    848\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtoken_type_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken_type_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    849\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    850\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    851\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    852\u001b[0m encoder_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencoder(\n\u001b[1;32m    853\u001b[0m     embedding_output,\n\u001b[1;32m    854\u001b[0m     attention_mask\u001b[38;5;241m=\u001b[39mextended_attention_mask,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    862\u001b[0m     return_dict\u001b[38;5;241m=\u001b[39mreturn_dict,\n\u001b[1;32m    863\u001b[0m )\n\u001b[1;32m    864\u001b[0m sequence_output \u001b[38;5;241m=\u001b[39m encoder_outputs[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/models/roberta/modeling_roberta.py:123\u001b[0m, in \u001b[0;36mRobertaEmbeddings.forward\u001b[0;34m(self, input_ids, token_type_ids, position_ids, inputs_embeds, past_key_values_length)\u001b[0m\n\u001b[1;32m    120\u001b[0m         token_type_ids \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mzeros(input_shape, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mlong, device\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mposition_ids\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m    122\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m inputs_embeds \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 123\u001b[0m     inputs_embeds \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mword_embeddings\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    124\u001b[0m token_type_embeddings \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtoken_type_embeddings(token_type_ids)\n\u001b[1;32m    126\u001b[0m embeddings \u001b[38;5;241m=\u001b[39m inputs_embeds \u001b[38;5;241m+\u001b[39m token_type_embeddings\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/sparse.py:162\u001b[0m, in \u001b[0;36mEmbedding.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    161\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 162\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membedding\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    163\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_norm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    164\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnorm_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscale_grad_by_freq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msparse\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/functional.py:2210\u001b[0m, in \u001b[0;36membedding\u001b[0;34m(input, weight, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse)\u001b[0m\n\u001b[1;32m   2204\u001b[0m     \u001b[38;5;66;03m# Note [embedding_renorm set_grad_enabled]\u001b[39;00m\n\u001b[1;32m   2205\u001b[0m     \u001b[38;5;66;03m# XXX: equivalent to\u001b[39;00m\n\u001b[1;32m   2206\u001b[0m     \u001b[38;5;66;03m# with torch.no_grad():\u001b[39;00m\n\u001b[1;32m   2207\u001b[0m     \u001b[38;5;66;03m#   torch.embedding_renorm_\u001b[39;00m\n\u001b[1;32m   2208\u001b[0m     \u001b[38;5;66;03m# remove once script supports set_grad_enabled\u001b[39;00m\n\u001b[1;32m   2209\u001b[0m     _no_grad_embedding_renorm_(weight, \u001b[38;5;28minput\u001b[39m, max_norm, norm_type)\n\u001b[0;32m-> 2210\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membedding\u001b[49m\u001b[43m(\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpadding_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscale_grad_by_freq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msparse\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 24.00 MiB (GPU 6; 47.54 GiB total capacity; 47.03 GiB already allocated; 23.69 MiB free; 47.20 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "rep_model.eval()\n",
    "level1_classification.eval()\n",
    "level2_classification.eval()\n",
    "level3_classification.eval()\n",
    "level1_correct, level2_correct, level3_correct = 0, 0, 0\n",
    "level1_correct_actual, level2_correct_actual, level3_correct_actual = 0, 0, 0\n",
    "level1_total, level2_total, level3_total = 0, 0, 0\n",
    "level1_total_actual, level2_total_actual, level3_total_actual = 0, 0, 0\n",
    "for train_batch in tqdm(test_loader):\n",
    "    rep = rep_model(train_batch['input_ids'].to(device),\n",
    "                                           train_batch['attention_mask'].to(device))\n",
    "    level1 = level1_classification(rep)\n",
    "    level2 = level2_classification(rep)\n",
    "    level3 = level3_classification(rep)\n",
    "    level1 = level1.argmax(-1)\n",
    "    level2 = level2.argmax(-1)\n",
    "    level3 = level3.argmax(-1)\n",
    "    \n",
    "    level1_correct += (level1 == train_batch['level1'].long().to(device)).sum().item()\n",
    "    level1_total += train_batch['level1'].shape[0]\n",
    "    level1_correct_actual += ((level1 == train_batch['level1'].long().to(device)) & (train_batch['level1'].long().to(device) != 7)).sum().item()\n",
    "    level1_total_actual += (train_batch['level1'].long().to(device) != 7).sum().item()\n",
    "    \n",
    "    level2_correct += (level2 == train_batch['level2'].long().to(device)).sum().item()\n",
    "    level2_total += train_batch['level2'].shape[0]\n",
    "    level2_correct_actual += ((level2 == train_batch['level2'].long().to(device)) & (train_batch['level2'].long().to(device) != 7)).sum().item()\n",
    "    level2_total_actual += (train_batch['level2'].long().to(device) != 15).sum().item()\n",
    "    \n",
    "    \n",
    "    level3_correct += (level3 == train_batch['level3'].long().to(device)).sum().item()\n",
    "    level3_total += train_batch['level3'].shape[0]\n",
    "    level3_correct_actual += ((level3 == train_batch['level3'].long().to(device)) & (train_batch['level3'].long().to(device) != 7)).sum().item()\n",
    "    level3_total_actual += (train_batch['level3'].long().to(device) != 7).sum().item()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "6fcda898-5d08-4e31-b5ca-4ea1201e2bf5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8615241635687733\n",
      "0.8068242166755177\n",
      "0.8855549654806161\n",
      "0.8645106140475851\n",
      "0.9415656008820287\n",
      "0.7830150345388054\n"
     ]
    }
   ],
   "source": [
    "print(level1_correct/level1_total)\n",
    "print(level2_correct/level2_total)\n",
    "print(level3_correct/level3_total)\n",
    "print(level1_correct_actual/level1_total_actual)\n",
    "print(level2_correct_actual/level2_total_actual)\n",
    "print(level3_correct_actual/level3_total_actual)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "7662bc35-2441-469a-88b3-69c0bace068d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(6, device='cuda:6')"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(level1 == train_batch['level1'].long().to(device)).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "5f5ae6af-7de0-4ee8-8139-1f9e8371640c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([12, 12, 11, 10,  1,  5, 11,  7], device='cuda:6')"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "level2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "208090b1-15bc-4919-9b9b-3c9002a8b128",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([12, 12,  6, 10,  1,  5, 11, 15])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_batch['level2']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "078f0ce5-2f9a-4d4c-9c56-129f470acf21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# comp.graphics\n",
    "# comp.os.ms-windows.misc\n",
    "# comp.sys.ibm.pc.hardware\n",
    "# comp.sys.mac.hardware\n",
    "# comp.windows.x\n",
    "# rec.autos\n",
    "# rec.motorcycles\n",
    "# rec.sport.baseball\n",
    "# rec.sport.hockey\n",
    "# sci.crypt\n",
    "# <sci.electronics\n",
    "# sci.med\n",
    "# sci.space\n",
    "# misc.forsale\n",
    "# talk.politics.misc\n",
    "# talk.politics.guns\n",
    "# talk.politics.mideast\n",
    "# talk.religion.misc\n",
    "# alt.atheism\n",
    "# soc.religion.christian\n",
    "# None\n",
    "level1 = [ \n",
    "    \"comp\", \"rec\", \"sci\", \"misc\", \"talk\", \"alt\", \"soc\", \"None\"\n",
    "]\n",
    "level2 = [\n",
    "    \"os\", \"sys\", \"windows\", \"graphics\", \"motorcycles\", \"sport\", \"autos\", \"religion\",\n",
    "    \"electronics\", \"med\", \"space\", \"forsale\", \"politics\", \"crypt\", \"None\"\n",
    "]\n",
    "level2_pass = {\n",
    "    \"soc\", \"alt\" \n",
    "}\n",
    "level3_pass = {\n",
    "    \"windows\", \"os\", \"religion\"\n",
    "}\n",
    "level3 = [\n",
    "    \"misc\", \"guns\", \"ibm\", \"mac\", \"baseball\", \"hockey\", \"mideast\", \"None\"\n",
    "]\n",
    "\n",
    "hierarchy = {\"comp\": {\"graphics\", \"os\", \"sys\", \"windows\"}, \"rec\": {\"auto\", \"motorcycles\", \"sport\"},\n",
    "            \"sci\": {\"crypt\", \"electronics\", \"med\", \"space\"}, \"misc\": {\"forsale\"}, \n",
    "             \"talk\": {\"politics\", \"religion\"}, \"alt\": {}, \"soc\": {}, \"None\": {},\n",
    "             \"windows\": {}, \"os\": {}, \"religion\": {}, \"politics\": {\"misc\", \"guns\", \"mideast\"},\n",
    "             \"sys\": {\"ibm\", \"mac\"}, \"sport\": {\"hockey\", \"baseball\"}\n",
    "            }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e2f4dc1-6cda-43a0-95d3-9cf371bfd255",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
