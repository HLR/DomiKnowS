{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cZFQu2AxytS4"
      },
      "source": [
        "# Email Spam detection\n",
        "This tutorial is to show you how to make a very simple learning program that also utilizes gorubi solver to apply constraints on a multiclass classification for two classes `spam` and `regular`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r3nK_bD7mFoX"
      },
      "source": [
        "!git clone https://github.com/HLR/DomiKnowS.git\n",
        "%cd DomiKnowS \n",
        "!git checkout origin/Tasks\n",
        "!pip install DomiKnowS\n",
        "\n",
        "import logging\n",
        "logging.basicConfig(level=logging.INFO)\n",
        "\n",
        "import __main__\n",
        "__main__.__file__=\"sentimentAnalysis.py\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UEhDkSXTq1H-",
        "outputId": "0bb5c295-cfca-428f-e880-d7b4fbe6ba50"
      },
      "source": [
        "import spacy.cli\n",
        "spacy.cli.download(\"en_core_web_lg\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[38;5;2mâœ” Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_lg')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T57ccdsynAIc"
      },
      "source": [
        "## The Graph\n",
        "First we define the graph code that defines the domain knowledge for this problem."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5yAEf2Ylm3yZ"
      },
      "source": [
        "import os,sys,inspect\n",
        "\n",
        "from domiknows.graph import Graph, Concept # importing basic graph classes\n",
        "from domiknows.graph.logicalConstrain import orL, andL, notL # importing basic constraint classes\n",
        "\n",
        "Graph.clear()\n",
        "Concept.clear()\n",
        "\n",
        "with Graph('example') as graph:\n",
        "    email = Concept(name='email')\n",
        "\n",
        "    Spam = email(name='spam')\n",
        "\n",
        "    Regular = email(name='regular')\n",
        "\n",
        "    # The constraint of not having regular and spam together\n",
        "    orL(andL(notL(Spam), Regular, andL(notL(Regular), Spam)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EXz886GenGOV"
      },
      "source": [
        "## Data and Data Reader\n",
        "As our data is located in different text files and in different folders, we have to write a reader class that reads this entries into a list of dictionaries in python. Here we use the default Reader class of the Framework.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7UU-Q0wsnKbE"
      },
      "source": [
        "import os\n",
        "from domiknows.data.reader import RegrReader\n",
        "\n",
        "class EmailSpamReader(RegrReader):\n",
        "    def parse_file(self, ):\n",
        "        folder = self.file\n",
        "        data_spam = []\n",
        "        data_ham = []\n",
        "        for file in [f for f in os.listdir(folder + \"/spam\") if os.path.isfile(os.path.join(folder + \"/spam\", f)) and f.endswith('.txt')]:\n",
        "            with open(folder + \"/spam/\" + file, \"r\") as f:\n",
        "                x = []\n",
        "                for i in f:\n",
        "                    x.append(i)\n",
        "            data_spam.append(x)\n",
        "        for file in [f for f in os.listdir(folder + \"/ham\") if os.path.isfile(os.path.join(folder + \"/ham\", f)) and f.endswith('.txt')]:\n",
        "            with open(folder + \"/ham/\" + file, \"r\") as f:\n",
        "                x = []\n",
        "                for i in f:\n",
        "                    x.append(i)\n",
        "            data_ham.append(x)\n",
        "        final_data = []\n",
        "        for dat in data_spam:\n",
        "            item = {'subject': dat[0].split(\":\")[1]}\n",
        "            index = [i for i, v in enumerate(dat) if v.startswith('- - - - - - - - -')]\n",
        "            if len(index):\n",
        "                index = index[0]\n",
        "                item['body'] = \"\".join(dat[1:index])\n",
        "                sub = [(i, v) for i, v in enumerate(dat[index:]) if v.startswith('subject')][0]\n",
        "                item['forward_subject'] = sub[1].split(\":\")[1]\n",
        "                item['forward_body'] = \"\".join(dat[index + sub[0] + 1:])\n",
        "            else:\n",
        "                item['body'] = item['body'] = (\"\").join(dat[1:])\n",
        "            item['label'] = \"spam\"\n",
        "            final_data.append(item)\n",
        "\n",
        "        for dat in data_ham:\n",
        "            item = {'subject': dat[0].split(\":\")[1]}\n",
        "            index = [i for i, v in enumerate(dat) if v.startswith('- - - - - - - - -')]\n",
        "            if len(index):\n",
        "                index = index[0]\n",
        "                item['body'] = \"\".join(dat[1:index])\n",
        "                sub = [(i, v) for i, v in enumerate(dat[index:]) if v.startswith('subject')][0]\n",
        "                item['forward_subject'] = sub[1].split(\":\")[1]\n",
        "                item['forward_body'] = \"\".join(dat[index + sub[0] + 1:])\n",
        "            else:\n",
        "                item['body'] = item['body'] = (\"\").join(dat[1:])\n",
        "            item['label'] = \"ham\"\n",
        "            final_data.append(item)\n",
        "        return final_data\n",
        "\n",
        "    def getSubjectval(self, item):\n",
        "        return item['subject']\n",
        "\n",
        "    def getBodyval(self, item):\n",
        "        return item['body']\n",
        "\n",
        "    def getForwardSubjectval(self, item):\n",
        "        if 'forward_subject' in item:\n",
        "            return item['forward_subject']\n",
        "        else:\n",
        "            return None\n",
        "\n",
        "    def getForwardBodyval(self, item):\n",
        "        if 'forward_body' in item:\n",
        "            return item['forward_body']\n",
        "        else:\n",
        "            return None\n",
        "\n",
        "    def getSpamval(self, item):\n",
        "        if item['label'] == \"spam\":\n",
        "            return [1]\n",
        "        else:\n",
        "            return [0]\n",
        "\n",
        "    def getRegularval(self, item):\n",
        "        if item['label'] == \"ham\":\n",
        "            return [1]\n",
        "        else:\n",
        "            return [0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ibA0IXstnRLD"
      },
      "source": [
        "This class redefines the `parse_file` function to parse data into a list of dictionary and then defines some keywords to be used by `ReaderSensor` later in our program to connect data with our knowledge graph. Next we make an instance of this class on the training samples."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DIsee7rRnU0f"
      },
      "source": [
        "import os\n",
        "\n",
        "train_reader = EmailSpamReader(file='Email_Spam/data/train', type=\"folder\")\n",
        "test_reader = EmailSpamReader(file='Email_Spam/data/test', type=\"folder\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1CgyvyBpnOVz"
      },
      "source": [
        "You can check your very first instance by calling `next` and your reader. \n",
        "! Make sure to re-initiate your reader if you do call `next` for test."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mmswtIpFnwXV",
        "outputId": "d1661d9d-1da2-4857-af8d-7b44862e578d"
      },
      "source": [
        "print(next(iter(train_reader)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'Body': \"i truely solicite your assistance for a business proposal .\\ndear friend ,\\nit is my absolute confidence to ensure this urgent and important businees proposal with you . i am greatly optimistic to forward you this note , as regards your assistance to enable me execute a venture of mutual benefit with you .\\nmy name is mr daniel mutade , a senior employee with the central bank of zimbabwe . during the last national election held by president robert mugabe , i and my colleagues worked out twenty million united states dollars ( us $ 20 m ) as over - invoiced and inflated payment for election materials and the fund is now deposited with a security company in europe for safe keeping . we are not sure of the future of our country zimbabwe , due to the cry of more sanctions by world leaders in and around the world as a result of the brutal take over of white farmers land and residents in zimbabwe by the mugabe ' s administration .\\nmy colleagues and i are now seeking to secure and invest this funds wisely abroad . i am currently in europe on a short visit and my involvement in this transaction shall be kept secret , since our civil service code of conduct forbids us to have any private account and financial dealings of this magnitude . for this reason , we need the assistance of a foreign company or person to push this money into their / his account .\\nyour share of assisting us successfully put intoyour account this moneywill be 26 % of the total sum , while 70 % will be for us ( my colleagues and i ) and 4 % has been mapped out for any expenses that may be incured by both parties during the cause of transferring the funds into your nominated bank account .\\ni will refrain from giving out more operational details until i urgentlyreceive your reply of interest , since time is of the essence in this transaction . however , i need to inform you that this transaction is 100 % risk free because we have done our homework carefully to ensure the smoothness and realization of this transaction .\\nplease if you consider this business necessary , do reply me as soon as possible for more details , as this transaction will usher us into a greater benefits within a shortest possible time . could you please send your reply to dmutade @ mail 2 dave . com please do include your telephone and fax numbers when replying to this business proposal .\\ntruely yours ,\\ndaniel mutade\\nexpn . com e - mail : http : / / expnmail . go . com\", 'ForwardBody': None, 'ForwardSubject': None, 'Regular': [0], 'Spam': [1], 'Subject': ' from mr . daniel mutade\\n'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QYBXxqaAn29a"
      },
      "source": [
        "## Model Declaration\n",
        "Now we start to connect the reader output data with our formatted domain knowledge defined in the graph."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6UYeXSSYn1NM"
      },
      "source": [
        "from domiknows.sensor.pytorch.sensors import ReaderSensor\n",
        "\n",
        "email['subject'] = ReaderSensor(keyword='Subject')\n",
        "email['body'] = ReaderSensor(keyword=\"Body\")\n",
        "email['forward_subject'] = ReaderSensor(keyword=\"ForwardSubject\")\n",
        "email['forward_body'] = ReaderSensor(keyword=\"ForwardBody\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "87au1onPn8JR"
      },
      "source": [
        "Next we read the labels for the `spam` and `regular` concepts"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EbKeXDW1n5fy"
      },
      "source": [
        "email[Spam] = ReaderSensor(keyword='Spam', label=True)\n",
        "email[Regular] = ReaderSensor(keyword='Regular', label=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7TYEeZHxoAuU"
      },
      "source": [
        "### Define a new sensor\n",
        "Here we want to use spacy to define a new sensor which gives us an average glove embedding tensor for a sentence"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bdF4aBf4n-eI"
      },
      "source": [
        "from domiknows.sensor.pytorch.sensors import TorchSensor, FunctionalSensor\n",
        "import spacy\n",
        "from typing import Any\n",
        "import torch\n",
        "import en_core_web_lg\n",
        "\n",
        "\n",
        "class SentenceRepSensor(FunctionalSensor):\n",
        "    def __init__(self, *pres, **kwarg):\n",
        "        super().__init__(*pres, **kwarg)\n",
        "        self.nlp = en_core_web_lg.load()\n",
        "\n",
        "    def forward(self, *inputs) -> Any:\n",
        "        email = self.nlp(inputs[0])\n",
        "        return torch.from_numpy(email.vector).to(device=self.device).unsqueeze(0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nMMA-1cSoHq5"
      },
      "source": [
        "The input to this sensor would be a sentence. You can find the usage of this sensor in the following sections.\n",
        "\n",
        "Next, we want to define a new sensor which gives us a tensor indicating whether the email has a forwarded message or not."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ec7uzFcJoFRY"
      },
      "source": [
        "def presence_detector(*inputs) -> Any:\n",
        "    if inputs[0] != None:\n",
        "        return torch.ones(1,1)\n",
        "    else:\n",
        "        return torch.zeros(1,1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GSKy-ehipWTj"
      },
      "source": [
        "### Connecting new sensors to the graph \n",
        "We connect these sensors to the graph to make new properties on the concept `email`. We want to make new representations on the `subject` and `body` of the email and that why those properties are passed as input to the defined sensors."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JlD9BmWDpT8o"
      },
      "source": [
        "email['subject_rep'] = SentenceRepSensor('subject')\n",
        "email['body_rep'] = SentenceRepSensor('body')\n",
        "email['forward_presence'] = FunctionalSensor('forward_body', forward=presence_detector)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9aU0WWo2pscb"
      },
      "source": [
        "### Preparing input features for the learner\n",
        "Now we concatenate all the generated features to make a new property on the graph which will provide input for the classifier of `spam` and `regular` concepts."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tYLOuArdpYgJ"
      },
      "source": [
        "email['features'] = FunctionalSensor('subject_rep', 'body_rep', 'forward_presence', forward=lambda *x : torch.cat((x), dim=-1))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EISxk_B-pypL"
      },
      "source": [
        "### Define the learner\n",
        "Here we define a learner and connect it to the concepts of `spam` and `regular`. This learner is a simple pytorch module of linear neural network."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FPapfj55p03r"
      },
      "source": [
        "from domiknows.sensor.pytorch.learners import ModuleLearner\n",
        "from torch import nn\n",
        "\n",
        "email[Spam] = ModuleLearner('features', module=nn.Linear(601, 2))\n",
        "email[Regular] = ModuleLearner('features', module=nn.Linear(601, 2))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DvJBNRs9p6IN"
      },
      "source": [
        "### Make the learning model from the updated graph\n",
        "Here we make an executable version of this graph that is able to trace the dependencies of the sensors and fill the data from the reader to run examples on the declared model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qnRGZKZTp3FY"
      },
      "source": [
        "from domiknows.program import POIProgram, IMLProgram, SolverPOIProgram\n",
        "from domiknows.program.model.pytorch import PoiModel\n",
        "from domiknows.program.metric import MacroAverageTracker, PRF1Tracker, PRF1Tracker, DatanodeCMMetric\n",
        "from domiknows.program.loss import NBCrossEntropyLoss\n",
        "\n",
        "program = SolverPOIProgram(graph, inferTypes=['ILP', 'local/argmax'], loss=MacroAverageTracker(NBCrossEntropyLoss()), metric={'ILP':PRF1Tracker(DatanodeCMMetric()),'argmax':PRF1Tracker(DatanodeCMMetric('local/argmax'))})\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4DeqBpvxp9fq"
      },
      "source": [
        "# set logger level to see training and testing logs\n",
        "import logging\n",
        "logging.basicConfig(level=logging.INFO)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ddn6pHibqBKV",
        "outputId": "81886e43-22b6-42f9-854c-390c9c5a9c6a"
      },
      "source": [
        "program.train(train_reader, train_epoch_num=10, Optim=torch.optim.Adam, device='auto')\n",
        "program.test(test_reader)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1 Training:  10%|â–ˆ         | 1/10 [00:00<00:04,  2.00it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Log file for ilpOntSolver is in: /content/DomiKnowS/logs/ilpOntSolver.log\n",
            "Log file for ilpOntSolverTime is in: /content/DomiKnowS/logs/ilpOntSolver.log\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1 Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:01<00:00,  5.58it/s]\n",
            "Epoch 2 Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:01<00:00,  7.02it/s]\n",
            "Epoch 3 Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:00<00:00, 12.10it/s]\n",
            "Epoch 4 Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:00<00:00, 12.17it/s]\n",
            "Epoch 5 Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:00<00:00, 12.41it/s]\n",
            "Epoch 6 Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:00<00:00, 12.16it/s]\n",
            "Epoch 7 Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:00<00:00, 12.53it/s]\n",
            "Epoch 8 Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:00<00:00, 12.28it/s]\n",
            "Epoch 9 Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:00<00:00, 12.63it/s]\n",
            "Epoch 10 Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:00<00:00, 12.43it/s]\n",
            "Testing: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:00<00:00, 10.03it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "REl_umejqKTE"
      },
      "source": [
        "## Run the graph\n",
        "Here we use populate to run the graph with the defined data from the reader"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uQRGe4wjqHJN",
        "outputId": "06a37336-809f-4483-f0af-45f9f5b9de8f"
      },
      "source": [
        "for datanode in program.populate(dataset=test_reader):\n",
        "    print('datanode:', datanode)\n",
        "    print('Spam:', datanode.getAttribute(Spam).softmax(-1))\n",
        "    print('Regular:', datanode.getAttribute(Regular).softmax(-1))\n",
        "#     datanode.inferILPResults(fun=lambda val: torch.tensor(val).softmax(dim=-1).detach().cpu().numpy().tolist(), epsilon=None)\n",
        "    print('inference spam:', datanode.getAttribute(Spam, 'ILP'))\n",
        "    print('inference regular:', datanode.getAttribute(Regular, 'ILP'))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2it [00:00, 17.00it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "datanode: email 0\n",
            "Spam: tensor([0.0329, 0.9671])\n",
            "Regular: tensor([0.8905, 0.1095])\n",
            "inference spam: tensor([1.])\n",
            "inference regular: tensor([0.])\n",
            "datanode: email 0\n",
            "Spam: tensor([0.3142, 0.6858])\n",
            "Regular: tensor([0.4669, 0.5331])\n",
            "inference spam: tensor([1.])\n",
            "inference regular: tensor([1.])\n",
            "datanode: email 0\n",
            "Spam: tensor([0.4158, 0.5842])\n",
            "Regular: tensor([0.4903, 0.5097])\n",
            "inference spam: tensor([1.])\n",
            "inference regular: tensor([1.])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "6it [00:00, 11.47it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "datanode: email 0\n",
            "Spam: tensor([0.3978, 0.6022])\n",
            "Regular: tensor([0.9004, 0.0996])\n",
            "inference spam: tensor([1.])\n",
            "inference regular: tensor([0.])\n",
            "datanode: email 0\n",
            "Spam: tensor([0.0504, 0.9496])\n",
            "Regular: tensor([0.9122, 0.0878])\n",
            "inference spam: tensor([1.])\n",
            "inference regular: tensor([0.])\n",
            "datanode: email 0\n",
            "Spam: tensor([0.4070, 0.5930])\n",
            "Regular: tensor([0.4993, 0.5007])\n",
            "inference spam: tensor([1.])\n",
            "inference regular: tensor([1.])\n",
            "datanode: email 0\n",
            "Spam: tensor([0.3122, 0.6878])\n",
            "Regular: tensor([0.8245, 0.1755])\n",
            "inference spam: tensor([1.])\n",
            "inference regular: tensor([0.])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "10it [00:00, 13.26it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "datanode: email 0\n",
            "Spam: tensor([0.7905, 0.2095])\n",
            "Regular: tensor([0.2728, 0.7272])\n",
            "inference spam: tensor([0.])\n",
            "inference regular: tensor([1.])\n",
            "datanode: email 0\n",
            "Spam: tensor([0.3657, 0.6343])\n",
            "Regular: tensor([0.8031, 0.1969])\n",
            "inference spam: tensor([1.])\n",
            "inference regular: tensor([0.])\n",
            "datanode: email 0\n",
            "Spam: tensor([0.9054, 0.0946])\n",
            "Regular: tensor([0.0896, 0.9104])\n",
            "inference spam: tensor([0.])\n",
            "inference regular: tensor([1.])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tMgy_fA0qPhX"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    }
  ]
}