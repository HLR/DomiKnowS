{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TVKopHWISxOa"
      },
      "outputs": [],
      "source": [
        "!git clone https://github.com/HLR/DomiKnowS.git\n",
        "%cd DomiKnowS\n",
        "!git checkout origin/Tasks\n",
        "!pip install DomiKnowS\n",
        "\n",
        "import logging\n",
        "logging.basicConfig(level=logging.INFO)\n",
        "\n",
        "import __main__\n",
        "__main__.__file__=\"graph.py\""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python -m spacy download en_core_web_sm\n",
        "!pip install torchtext==0.9 --no-dependencies"
      ],
      "metadata": {
        "id": "03XmVVRKS-EL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "First, build the graph that specifies the domain knowledge for this problem"
      ],
      "metadata": {
        "id": "1VnL4BgPTSuu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import logging\n",
        "\n",
        "logging.basicConfig(level=logging.INFO)\n",
        "from domiknows.graph import Graph, Concept, Relation\n",
        "from domiknows.graph.relation import disjoint\n",
        "\n",
        "Graph.clear()\n",
        "Concept.clear()\n",
        "Relation.clear()\n",
        "\n",
        "with Graph('global') as graph:\n",
        "  review = Concept(name='review')\n",
        "\n",
        "  positive = review(name='positive')\n",
        "  negative = review(name='negative')\n",
        "\n",
        "  disjoint(positive, negative)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UyBKv_RfTKdo",
        "outputId": "2911b067-e240-4430-faa7-65824f98aeba"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Log file for dataNode is in: /content/DomiKnowS/logs/datanode.log\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Define the readers for the labels and text\n",
        "\n"
      ],
      "metadata": {
        "id": "5QV24hbYTi7o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from domiknows.sensor.pytorch.sensors import ReaderSensor\n",
        "\n",
        "review['text'] = ReaderSensor(keyword='text')\n",
        "\n",
        "review[positive] = ReaderSensor(keyword='positive', label=True)\n",
        "review[negative] = ReaderSensor(keyword='negative', label=True)"
      ],
      "metadata": {
        "id": "GNcJlluYTw84"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Define the model parameters"
      ],
      "metadata": {
        "id": "ypUO7PfdTyKP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "embed_size = 300\n",
        "hidden_size = 100\n",
        "num_classes = 2\n",
        "drop_rate = 0.5"
      ],
      "metadata": {
        "id": "j-ZJ31AiT5Tn"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Define a sensor that converts the raw text to GloVe embedding vectors and connect the sensor to the graph such that it creates the embedding representation of the text based on the raw data."
      ],
      "metadata": {
        "id": "yjXI7J6qT7kn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from domiknows.sensor.pytorch.sensors import FunctionalSensor\n",
        "from torchtext.vocab import GloVe\n",
        "from torchtext.data.utils import get_tokenizer\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "import torch\n",
        "\n",
        "class EmbeddingSensor(FunctionalSensor):\n",
        "  def __init__(self, *pres, **kwarg):\n",
        "    super().__init__(*pres, **kwarg)\n",
        "\n",
        "    self.vocab = GloVe(name='840B', dim=embed_size)\n",
        "    self.tokenizer = get_tokenizer('spacy', language='en')\n",
        "\n",
        "  def forward(self, *inputs):\n",
        "    text = inputs[0]\n",
        "\n",
        "    tokens_batch = [self.tokenizer(text)]\n",
        "\n",
        "    emb_batch = []\n",
        "    for tokens in tokens_batch:\n",
        "      rev_emb = torch.empty((len(tokens), embed_size))\n",
        "      for i, tok in enumerate(tokens):\n",
        "        rev_emb[i] = self.vocab[tok]\n",
        "\n",
        "      emb_batch.append(rev_emb)\n",
        "\n",
        "    padded = pad_sequence(emb_batch)\n",
        "\n",
        "    out = padded.to(device=self.device)\n",
        "\n",
        "    return out\n",
        "\n",
        "review['text_embed'] = EmbeddingSensor('text')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uNrsY7K2T_Uw",
        "outputId": "553b7af5-e122-4b6b-82ee-9cf6bc8a600c"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            ".vector_cache/glove.840B.300d.zip: 2.18GB [06:50, 5.30MB/s]                            \n",
            "100%|█████████▉| 2196016/2196017 [05:34<00:00, 6558.57it/s]\n",
            "/usr/local/lib/python3.10/dist-packages/torchtext/data/utils.py:105: UserWarning: Spacy model \"en\" could not be loaded, trying \"en_core_web_sm\" instead\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Define a pytorch module for the LSTM model to produce RNN representations of the text\n"
      ],
      "metadata": {
        "id": "p4NDi59xUBu3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch import nn\n",
        "\n",
        "class LSTMModule(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(LSTMModule, self).__init__()\n",
        "\n",
        "    self.rnn = nn.LSTM(embed_size, hidden_size, bidirectional=True)\n",
        "    self.dropout = nn.Dropout(p=drop_rate)\n",
        "\n",
        "  def forward(self, input):\n",
        "    output, (h, c) = self.rnn(input)\n",
        "    forward, backward = torch.chunk(output, 2, dim=2)\n",
        "    comb = torch.cat((forward[-1,:,:], backward[0,:,:]), dim=1)\n",
        "\n",
        "    return self.dropout(comb)"
      ],
      "metadata": {
        "id": "ZWQdi05AUXV4"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Define a learner using the previously specified pytorch module to create the RNN representation.\n",
        "\n",
        "Then, specify learners that use a pytorch linear neural network to perform predictions based on that RNN representation."
      ],
      "metadata": {
        "id": "H2MdMacyUYJI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from domiknows.sensor.pytorch.learners import ModuleLearner\n",
        "from torch import nn\n",
        "\n",
        "review['rnn_embed'] = ModuleLearner('text_embed', module=LSTMModule())\n",
        "\n",
        "review[positive] = ModuleLearner('rnn_embed', module=nn.Linear(hidden_size * 2, num_classes))\n",
        "review[negative] = ModuleLearner('rnn_embed', module=nn.Linear(hidden_size * 2, num_classes))"
      ],
      "metadata": {
        "id": "pfolewBXUZ2Q"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Define a learnable model from the previously specified graph."
      ],
      "metadata": {
        "id": "8pADRQQzUhoY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from domiknows.program import SolverPOIProgram\n",
        "from domiknows.program.metric import MacroAverageTracker, PRF1Tracker, DatanodeCMMetric\n",
        "from domiknows.program.loss import NBCrossEntropyLoss\n",
        "\n",
        "program = SolverPOIProgram(graph, inferTypes=['ILP', 'local/argmax'], loss=MacroAverageTracker(NBCrossEntropyLoss()), metric={'ILP':PRF1Tracker(DatanodeCMMetric()),'argmax':PRF1Tracker(DatanodeCMMetric('local/argmax'))})"
      ],
      "metadata": {
        "id": "Btun6FY1UkFP"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Load the IMDB data\n",
        "\n"
      ],
      "metadata": {
        "id": "fzmhSS2cUnEA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz -P data/\n",
        "!echo 'Extracting aclImdb_v1.tar.gz...'\n",
        "!tar -xzf data/aclImdb_v1.tar.gz -C data/\n",
        "\n",
        "import glob\n",
        "import random\n",
        "import os\n",
        "\n",
        "def get_data(directory, label):\n",
        "  data_all = []\n",
        "  for path in glob.glob(os.path.join(directory, label + '/*.txt')):\n",
        "    data_dict = {}\n",
        "    with open(path, 'r') as f:\n",
        "      data_dict['text'] = f.read()\n",
        "      data_dict['positive'] = [1 if label == 'pos' else 0]\n",
        "      data_dict['negative'] = [1 if label == 'neg' else 0]\n",
        "    data_all.append(data_dict)\n",
        "  return data_all\n",
        "\n",
        "train_data = get_data('data/aclImdb/train', 'pos')\n",
        "train_data.extend(get_data('data/aclImdb/train', 'neg'))\n",
        "random.shuffle(train_data)\n",
        "\n",
        "test_data = get_data('data/aclImdb/test', 'pos')\n",
        "test_data.extend(get_data('data/aclImdb/test', 'neg'))\n",
        "random.shuffle(test_data)\n",
        "\n",
        "\n",
        "print(train_data[0])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-PwXXaMYTtc5",
        "outputId": "420076d5-2763-44f3-f31d-ea0bc4097650"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-07-11 19:36:04--  http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\n",
            "Resolving ai.stanford.edu (ai.stanford.edu)... 171.64.68.10\n",
            "Connecting to ai.stanford.edu (ai.stanford.edu)|171.64.68.10|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 84125825 (80M) [application/x-gzip]\n",
            "Saving to: ‘data/aclImdb_v1.tar.gz’\n",
            "\n",
            "aclImdb_v1.tar.gz   100%[===================>]  80.23M  23.7MB/s    in 3.4s    \n",
            "\n",
            "2023-07-11 19:36:08 (23.7 MB/s) - ‘data/aclImdb_v1.tar.gz’ saved [84125825/84125825]\n",
            "\n",
            "Extracting aclImdb_v1.tar.gz...\n",
            "{'text': \"Now I recently had the viewing pleasure to watch the hilarious comedy Bachelor Party, one of my new favorite comedies, laughed until it just hurt type of movies. So I naturally wanted to see the sequel, hoping it would have the same laughs, but instead Bachelor Party 2: The Last Temptation is made by the American Pie generation where it's tasteless and defeats the hole purpose of the first film. Yeah, the first film has nudity, but it doesn't show in every single scene. Also the plot is exactly the same from the first, it's not always a complaint with me, but this could have been a little more original. The only thing is that I'm glad that at least no old actors from the original appear in this movie, because it would have been cheesy or really silly looking.<br /><br />Ron and Melinda are engaged, after only 2 months of dating, everyone is against it. Melinda has a rich family, but they're pretty happy with Ron, and Melinda's brother, Todd is scared that Ron will take his job. So they go out on a weekend to Miami for a bachelor party and Todd is going to make sure that he'll trap Ron into a picture that will make Melinda change her mind about the marriage.<br /><br />Bachelor Party 2: The Last Temptation has a couple laughs here and there, but over all fails to deliver what the first film accomplished. These guys, Ron's friends, were more obnoxious than likable, except for Seth, he was kinda funny. The only likable characters other than Seth is Ron and Melinda, everyone else just more or less gets on your nerves. You wanna watch this film? Just watch Girls Gone Wild, it's the same thing only it doesn't try to pretend that it's a film. Stick to the original Bachelor Party, that's the movie that's going to get you in tears of laughter.<br /><br />3/10\", 'positive': [0], 'negative': [1]}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Train the model"
      ],
      "metadata": {
        "id": "L7hfODULYhIz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "split_idx = int(len(train_data)*0.8)\n",
        "\n",
        "program.train(train_data[:5000],\n",
        "              valid_set=train_data[split_idx:split_idx+2000],\n",
        "              test_set=test_data[:2000], train_epoch_num=10, Optim=torch.optim.Adam, device='cuda')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IQYbwfM-YhWh",
        "outputId": "e03c41b0-9381-4d6c-9c7e-2ec37c4c9814"
      },
      "execution_count": 12,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\rEpoch 1 Training:   0%|          | 0/5000 [00:00<?, ?it/s]"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Log file for ilpOntSolver is in: /content/DomiKnowS/logs/ilpOntSolver.log\n",
            "Log file for ilpOntSolverTime is in: /content/DomiKnowS/logs/ilpOntSolver.log\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1 Training: 100%|██████████| 5000/5000 [04:58<00:00, 16.78it/s]\n",
            "Epoch 1 Validation: 100%|██████████| 2000/2000 [01:48<00:00, 18.43it/s]\n",
            "Epoch 2 Training: 100%|██████████| 5000/5000 [04:42<00:00, 17.69it/s]\n",
            "Epoch 2 Validation: 100%|██████████| 2000/2000 [01:38<00:00, 20.30it/s]\n",
            "Epoch 3 Training: 100%|██████████| 5000/5000 [04:53<00:00, 17.04it/s]\n",
            "Epoch 3 Validation: 100%|██████████| 2000/2000 [01:45<00:00, 18.90it/s]\n",
            "Epoch 4 Training: 100%|██████████| 5000/5000 [04:43<00:00, 17.61it/s]\n",
            "Epoch 4 Validation: 100%|██████████| 2000/2000 [01:43<00:00, 19.31it/s]\n",
            "Epoch 5 Training: 100%|██████████| 5000/5000 [04:49<00:00, 17.28it/s]\n",
            "Epoch 5 Validation: 100%|██████████| 2000/2000 [01:46<00:00, 18.70it/s]\n",
            "Epoch 6 Training: 100%|██████████| 5000/5000 [04:55<00:00, 16.94it/s]\n",
            "Epoch 6 Validation: 100%|██████████| 2000/2000 [01:45<00:00, 18.87it/s]\n",
            "Epoch 7 Training: 100%|██████████| 5000/5000 [04:49<00:00, 17.28it/s]\n",
            "Epoch 7 Validation: 100%|██████████| 2000/2000 [01:46<00:00, 18.84it/s]\n",
            "Epoch 8 Training: 100%|██████████| 5000/5000 [04:53<00:00, 17.02it/s]\n",
            "Epoch 8 Validation: 100%|██████████| 2000/2000 [01:49<00:00, 18.25it/s]\n",
            "Epoch 9 Training: 100%|██████████| 5000/5000 [05:00<00:00, 16.63it/s]\n",
            "Epoch 9 Validation: 100%|██████████| 2000/2000 [01:49<00:00, 18.23it/s]\n",
            "Epoch 10 Training: 100%|██████████| 5000/5000 [04:49<00:00, 17.27it/s]\n",
            "Epoch 10 Validation: 100%|██████████| 2000/2000 [01:48<00:00, 18.43it/s]\n",
            "Epoch 10 Testing: 100%|██████████| 2000/2000 [01:44<00:00, 19.08it/s]\n"
          ]
        }
      ]
    }
  ]
}