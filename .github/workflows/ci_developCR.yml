name: CI - Continuous Integration for develop-CLEVER-relations  Branch
on:
  push: { branches: [develop-CLEVER-relations] }
  pull_request: { branches: [develop-CLEVER-relations] }

jobs:
  test:
    runs-on: ubuntu-latest
    steps:
      # Checks-out the repository branch develop-CLEVER-relations under $GITHUB_WORKSPACE, so the workflow job can access it
      - uses: actions/checkout@v4
        with:
          ref: "develop-CLEVER-relations"

      - name: Set up Python
        uses: actions/setup-python@v5
        with: { python-version: "3.13" }

      - name: Install uv
        uses: astral-sh/setup-uv@v4
        with:
          enable-cache: true

      - name: Resolve & install
        run: |
         uv sync --extra cpu --extra dev 

      - name: Run regression tests
        run: |
          # List of specific test subfolders to run from test_regr
          # Add subfolder names to this array as needed
          TEST_LIST=(
            'Clever'
            'InferenceAPI'
            'dummy_datanode'
            'examples/PMDExistL'
            'examples/conll04'
            'examples/edges'
            'examples/equality'
            'examples/graph_city'
            'examples/graph_coloring'
            'examples/logical_all'
            'examples/mnist-arithmetic'
            'examples/nested_relations'
            'examples/orbs'
            'examples/test_switch_constraints'
            'graph'
            'graph_errors'
            'sensor'
            'sensor/pytorch'
            'simple_regression'
            'solver'
          )
          
          if [ ${#TEST_LIST[@]} -eq 0 ]; then
            echo "No test subfolders specified in TEST_LIST. Skipping test execution."
            exit 0
          fi
          
          TEST_BASE_DIR="test_regr"
          echo "Using test base directory: $TEST_BASE_DIR"
          
          # Show available test directories for debugging
          echo "Available test directories in $TEST_BASE_DIR:"
          find "$TEST_BASE_DIR" -type d -name "*" | sort
          
          # Track overall test result and detailed failure information
          OVERALL_RESULT=0
          declare -A FAILED_TESTS
          declare -A TEST_OUTPUTS
          
          # Run tests from specified subfolders
          for subfolder in "${TEST_LIST[@]}"; do
            test_path="$TEST_BASE_DIR/$subfolder"
            echo "Running tests in $test_path"
            
            if [ -d "$test_path" ]; then
              # Check if there are any test files in the directory
              test_files_count=$(find "$test_path" -name "test_*.py" -o -name "*_test.py" | wc -l)
              
              # Capture both stdout and stderr, with detailed pytest output
              test_output=$(uv run pytest -v --tb=short --no-header "$test_path" 2>&1)
              test_exit_code=$?
              
              if [ $test_exit_code -eq 5 ]; then
                # Exit code 5: No tests collected
                if [ $test_files_count -eq 0 ]; then
                  echo "‚ö†Ô∏è  No test files found in $test_path (skipping)"
                  failure_reason="No test files found (*.py files matching test_* or *_test pattern)"
                else
                  echo "‚ö†Ô∏è  No tests collected in $test_path (test files exist but no tests discovered)"
                  failure_reason="No tests collected - test files exist but pytest couldn't discover any tests"
                fi
                FAILED_TESTS["$subfolder"]="$failure_reason"
                TEST_OUTPUTS["$subfolder"]="$test_output"
                OVERALL_RESULT=1
              elif [ $test_exit_code -ne 0 ]; then
                OVERALL_RESULT=1
                
                # Extract failure summary from pytest output
                failure_summary=$(echo "$test_output" | grep -A 10 "short test summary info" | tail -n +2 | head -20)
                if [ -z "$failure_summary" ]; then
                  # Fallback: look for FAILED lines
                  failure_summary=$(echo "$test_output" | grep "FAILED\|ERROR\|Exception:" | head -10)
                fi
                if [ -z "$failure_summary" ]; then
                  # Last resort: get last few lines of output
                  failure_summary=$(echo "$test_output" | tail -10)
                fi
                
                FAILED_TESTS["$subfolder"]="$failure_summary"
                TEST_OUTPUTS["$subfolder"]="$test_output"
                echo "‚ùå Tests FAILED in $test_path (exit code: $test_exit_code)"
                echo "Failure details:"
                echo "$failure_summary"
                echo "---"
              else
                echo "‚úÖ Tests PASSED in $test_path"
              fi
            else
              echo "Warning: Directory $test_path not found, skipping..."
              OVERALL_RESULT=1
              FAILED_TESTS["$subfolder"]="Directory not found"
            fi
          done
          
          # Report final results with detailed failure information
          echo "================================="
          echo "DETAILED TEST RESULTS SUMMARY"
          echo "================================="
          
          if [ $OVERALL_RESULT -eq 0 ]; then
            echo "‚úÖ ALL TESTS PASSED"
          else
            echo "‚ùå SOME TESTS FAILED"
            echo ""
            
            for subfolder in "${!FAILED_TESTS[@]}"; do
              echo "üìÅ FAILED: $subfolder"
              echo "   Reason: ${FAILED_TESTS[$subfolder]}"
              echo ""
            done
            
            # Optional: Show full output for debugging (can be commented out to reduce noise)
            echo "================================="
            echo "FULL TEST OUTPUTS (for debugging)"
            echo "================================="
            for subfolder in "${!TEST_OUTPUTS[@]}"; do
              echo "üìÅ Full output for $subfolder:"
              echo "${TEST_OUTPUTS[$subfolder]}"
              echo "---"
            done
            
            echo "================================="
            exit 1
          fi