name: CI - Continuous Integration for develop-CLEVER-relations  Branch
on:
  push: { branches: [develop-CLEVER-relations] }
  pull_request: { branches: [develop-CLEVER-relations] }

jobs:
  test:
    runs-on: ubuntu-latest
    steps:
      # Checks-out the repository branch develop-CLEVER-relations under $GITHUB_WORKSPACE, so the workflow job can access it
      - uses: actions/checkout@v4
        with:
          ref: "develop-CLEVER-relations"

      - name: Set up Python
        uses: actions/setup-python@v5
        with: { python-version: "3.13" }

      - name: Install uv
        uses: astral-sh/setup-uv@v4
        with:
          enable-cache: true

      - name: Resolve & install
        run: |
         uv sync --extra cpu --extra dev 

      - name: Run regression tests
        id: run_tests
        run: |
          set +e  # Don't exit on first error - we want to handle exit codes manually
          
          # List of specific test subfolders to run from test_regr
          # Add subfolder names to this array as needed
          TEST_LIST=(
            'Clever'
            'InferenceAPI'
            'dummy_datanode'
            'examples/PMDExistL'
            'examples/conll04'
            'examples/edges'
            'examples/equality'
            'examples/graph_city'
            'examples/graph_coloring'
            'examples/logical_all'
            'examples/mnist-arithmetic'
            'examples/nested_relations'
            'examples/orbs'
            'examples/test_switch_constraints'
            'graph'
            'graph_errors'
            'sensor'
            'sensor/pytorch'
            'simple_regression'
            'solver'
          )
          
          if [ ${#TEST_LIST[@]} -eq 0 ]; then
            echo "No test subfolders specified in TEST_LIST. Skipping test execution."
            exit 0
          fi
          
          TEST_BASE_DIR="test_regr"
          echo "Using test base directory: $TEST_BASE_DIR"
          
          # Show available test directories for debugging
          echo "Available test directories in $TEST_BASE_DIR:"
          find "$TEST_BASE_DIR" -type d -name "*" | sort
          
          # Track overall test result and detailed failure information
          OVERALL_RESULT=0
          declare -A FAILED_TESTS
          declare -A TEST_OUTPUTS
          declare -A SKIPPED_TESTS
          
          # Create results file for the report step
          RESULTS_FILE="/tmp/test_results.txt"
          echo "" > "$RESULTS_FILE"
          
          # Run tests from specified subfolders
          for subfolder in "${TEST_LIST[@]}"; do
            test_path="$TEST_BASE_DIR/$subfolder"
            echo "============================================"
            echo "üîç DEBUGGING: Processing $subfolder"
            echo "   Full path: $test_path"
            
            # Check if directory exists
            if [ -d "$test_path" ]; then
              echo "   ‚úÖ Directory exists"
              
              # List all files in the directory for debugging
              echo "   üìÅ Directory contents:"
              ls -la "$test_path" || echo "   ‚ùå Failed to list directory contents"
              
              # Check if there are any test files in the directory
              echo "   üîç Looking for test files..."
              test_files=$(find "$test_path" -name "test_*.py" -o -name "*_test.py" 2>/dev/null)
              if [ -n "$test_files" ]; then
                test_files_count=$(echo "$test_files" | wc -l)
              else
                test_files_count=0
              fi
              
              if [ $test_files_count -gt 0 ]; then
                echo "   üìù Found $test_files_count test file(s):"
                echo "$test_files"
              else
                echo "   ‚ö†Ô∏è  No test files found matching patterns test_*.py or *_test.py"
                
                # Check for any Python files
                py_files=$(find "$test_path" -name "*.py" 2>/dev/null)
                if [ -n "$py_files" ]; then
                  py_files_count=$(echo "$py_files" | wc -l)
                  echo "   üìù Found $py_files_count Python file(s) (but not matching test patterns):"
                  echo "$py_files"
                else
                  py_files_count=0
                  echo "   üìù No Python files found at all"
                fi
              fi
              
              echo "   üß™ Running pytest..."
              echo "   Command: uv run pytest -v --tb=short --no-header \"$test_path\""
              
              # Capture both stdout and stderr, with detailed pytest output
              test_output=$(uv run pytest -v --tb=short --no-header "$test_path" 2>&1)
              test_exit_code=$?
              
              echo "   üìä Pytest exit code: $test_exit_code"
              echo "   üìÑ Pytest output length: $(echo "$test_output" | wc -l) lines"
              echo "   üìÑ First few lines of pytest output:"
              echo "$test_output" | head -5
              
              if [ $test_exit_code -eq 5 ]; then
                # Exit code 5: No tests collected - treat as warning, not failure
                echo "   ‚ö†Ô∏è  Exit code 5: No tests collected"
                if [ $test_files_count -eq 0 ]; then
                  echo "   üìù Reason: No test files found in directory"
                  skip_reason="No test files found (*.py files matching test_* or *_test pattern)"
                else
                  echo "   üìù Reason: Test files exist but no tests discovered by pytest"
                  skip_reason="No tests collected - test files exist but pytest couldn't discover any tests"
                fi
                SKIPPED_TESTS["$subfolder"]="$skip_reason"
                echo "SKIP:$subfolder:$skip_reason" >> "$RESULTS_FILE"
                echo "   ‚úÖ Treating as SKIP (not failure)"
                # Don't set OVERALL_RESULT=1 for exit code 5 - treat as skip, not failure
              elif [ $test_exit_code -ne 0 ]; then
                echo "   ‚ùå Exit code $test_exit_code: Test failure"
                OVERALL_RESULT=1
                
                # Extract failure summary from pytest output
                failure_summary=$(echo "$test_output" | grep -A 10 "short test summary info" | tail -n +2 | head -20)
                if [ -z "$failure_summary" ]; then
                  # Fallback: look for FAILED lines
                  failure_summary=$(echo "$test_output" | grep "FAILED\|ERROR\|Exception:" | head -10)
                fi
                if [ -z "$failure_summary" ]; then
                  # Last resort: get last few lines of output
                  failure_summary=$(echo "$test_output" | tail -10)
                fi
                
                FAILED_TESTS["$subfolder"]="$failure_summary"
                TEST_OUTPUTS["$subfolder"]="$test_output"
                echo "FAIL:$subfolder:$failure_summary" >> "$RESULTS_FILE"
                echo "   üìù Failure details preview:"
                echo "$failure_summary" | head -3
              else
                echo "   ‚úÖ Exit code 0: Tests PASSED"
                echo "PASS:$subfolder:" >> "$RESULTS_FILE"
              fi
            else
              echo "   ‚ùå Directory does not exist: $test_path"
              SKIPPED_TESTS["$subfolder"]="Directory not found"
              echo "SKIP:$subfolder:Directory not found" >> "$RESULTS_FILE"
            fi
            echo "============================================"
          done
          
          # Save overall result for the report step
          echo "OVERALL_RESULT=$OVERALL_RESULT" >> "$RESULTS_FILE"
          
          # Always exit 0 during debugging phase
          echo "üöß DEBUG MODE: CI set to always pass (not failing on test failures)"
          exit 0

      - name: Test Results Report
        if: always()
        run: |
          set +e  # Don't exit on any command failures
          
          RESULTS_FILE="/tmp/test_results.txt"
          
          if [ ! -f "$RESULTS_FILE" ]; then
            echo "‚ùå No test results file found!"
            echo "‚úÖ Report completed (debug mode - always passing)"
            exit 0
          fi
          
          # Parse results
          passed_count=0
          failed_count=0
          skipped_count=0
          
          declare -A FAILED_TESTS
          declare -A SKIPPED_TESTS
          
          while IFS=':' read -r status subfolder reason; do
            case "$status" in
              "PASS")
                ((passed_count++))
                ;;
              "FAIL")
                ((failed_count++))
                FAILED_TESTS["$subfolder"]="$reason"
                ;;
              "SKIP")
                ((skipped_count++))
                SKIPPED_TESTS["$subfolder"]="$reason"
                ;;
              "OVERALL_RESULT"*)
                OVERALL_RESULT=$(echo "$status" | cut -d'=' -f2)
                ;;
            esac
          done < "$RESULTS_FILE"
          
          total_tests=$((passed_count + failed_count + skipped_count))
          
          echo "================================="
          echo "üìä FINAL TEST RESULTS SUMMARY"
          echo "================================="
          
          echo "üìä SUMMARY: $total_tests total directories"
          echo "   ‚úÖ $passed_count passed"
          echo "   ‚ùå $failed_count failed" 
          echo "   ‚ö†Ô∏è  $skipped_count skipped"
          echo ""
          
          if [ $failed_count -eq 0 ]; then
            echo "üéâ ALL TESTS PASSED OR SKIPPED"
            
            # Show skipped tests if any
            if [ ${#SKIPPED_TESTS[@]} -gt 0 ]; then
              echo ""
              echo "üìã SKIPPED TEST DIRECTORIES:"
              for subfolder in "${!SKIPPED_TESTS[@]}"; do
                echo "   ‚ö†Ô∏è  $subfolder: ${SKIPPED_TESTS[$subfolder]}"
              done
            fi
          else
            echo "üí• TEST FAILURES DETECTED"
            echo ""
            
            for subfolder in "${!FAILED_TESTS[@]}"; do
              echo "‚ùå FAILED: $subfolder"
              # Clean up the failure reason - extract the key information
              failure_reason="${FAILED_TESTS[$subfolder]}"
              if [[ "$failure_reason" == *"Exception:"* ]]; then
                # Extract just the exception part
                exception_line=$(echo "$failure_reason" | grep "Exception:" | head -1)
                echo "   üíÄ $exception_line"
              elif [[ "$failure_reason" == *"FAILED"* ]]; then
                # Extract just the FAILED line
                failed_line=$(echo "$failure_reason" | grep "FAILED" | head -1)
                echo "   üíÄ $failed_line"
              else
                echo "   üíÄ ${failure_reason}"
              fi
              echo ""
            done
            
            # Show skipped tests if any
            if [ ${#SKIPPED_TESTS[@]} -gt 0 ]; then
              echo "üìã SKIPPED TEST DIRECTORIES (not failures):"
              for subfolder in "${!SKIPPED_TESTS[@]}"; do
                echo "   ‚ö†Ô∏è  $subfolder: ${SKIPPED_TESTS[$subfolder]}"
              done
              echo ""
            fi
            
            echo "üöß DEBUG MODE: CI configured to not fail on test failures"
            echo "   Real failures are logged above but won't trigger notifications"
            echo "   To restore normal CI behavior, uncomment the exit 1 in the workflow"
            # exit 1  # Commented out - don't fail CI during debugging
          fi
          
          # Always exit 0 during debug mode
          echo ""
          echo "‚úÖ Report completed (debug mode - always passing)"
          exit 0