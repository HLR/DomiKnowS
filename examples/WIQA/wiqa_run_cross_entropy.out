* Owlready2 * Warning: optimized Cython parser module 'owlready2_optimized' is not available, defaulting to slower Python implementation
/data/hlr/chenzheng/data/domiknows_project/DomiKnowS/examples/WIQA/WIQA_aug.py:94: UserWarning: Please use OrderedDict rather than dict to prevent unpredictable order of arguments.For this instance, OrderedDict([('arg1', 'question'), ('arg2', 'question')]) is used.
  s_arg1, s_arg2 = symmetric.has_a(arg1=question, arg2=question)
/data/hlr/chenzheng/data/domiknows_project/DomiKnowS/examples/WIQA/WIQA_aug.py:106: UserWarning: Please use OrderedDict rather than dict to prevent unpredictable order of arguments.For this instance, OrderedDict([('arg11', 'question'), ('arg22', 'question'), ('arg33', 'question')]) is used.
  t_arg1, t_arg2, t_arg3 = transitive.has_a(arg11=question, arg22=question, arg33=question)
Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.bias', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight']
- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
INFO:root:training and testing begins
INFO:regr.program.program:Epoch: 1
INFO:regr.program.program:Training:
Log file for dataNode is in: /data/hlr/chenzheng/data/domiknows_project/DomiKnowS/examples/WIQA/logs/datanode.log
Graph Declaration:
Sensor Part:
simple program
this epoch is number: 0 &&&&&&&&&&
Epoch 1 Training:   0%|          | 0/7890 [00:00<?, ?it/s]Log file for ilpOntSolver is in: /data/hlr/chenzheng/data/domiknows_project/DomiKnowS/examples/WIQA/logs/ilpOntSolver.log
Log file for ilpOntSolverTime is in: /data/hlr/chenzheng/data/domiknows_project/DomiKnowS/examples/WIQA/logs/ilpOntSolver.log
hello
Academic license - for non-commercial use only - expires 2023-02-10
INFO:gurobipy.gurobipy:Academic license - for non-commercial use only - expires 2023-02-10
Using license file /home/zhengchen/gurobi.lic
INFO:gurobipy.gurobipy:Using license file /home/zhengchen/gurobi.lic
Epoch 1 Training:   0%|          | 1/7890 [00:00<32:25,  4.06it/s]Epoch 1 Training:   0%|          | 3/7890 [00:00<21:37,  6.08it/s]Epoch 1 Training:   0%|          | 4/7890 [00:00<22:49,  5.76it/s]Epoch 1 Training:   0%|          | 5/7890 [00:00<23:36,  5.56it/s]Epoch 1 Training:   0%|          | 6/7890 [00:01<23:57,  5.48it/s]Epoch 1 Training:   0%|          | 7/7890 [00:01<23:34,  5.57it/s]Epoch 1 Training:   0%|          | 9/7890 [00:01<20:27,  6.42it/s]Epoch 1 Training:   0%|          | 10/7890 [00:01<21:02,  6.24it/s]Epoch 1 Training:   0%|          | 11/7890 [00:01<21:37,  6.07it/s]Epoch 1 Training:   0%|          | 12/7890 [00:02<22:05,  5.94it/s]Epoch 1 Training:   0%|          | 13/7890 [00:02<22:05,  5.94it/s]Epoch 1 Training:   0%|          | 14/7890 [00:02<22:14,  5.90it/s]Epoch 1 Training:   0%|          | 15/7890 [00:02<22:53,  5.74it/s]Epoch 1 Training:   0%|          | 16/7890 [00:02<22:54,  5.73it/s]Epoch 1 Training:   0%|          | 17/7890 [00:02<22:49,  5.75it/s]Epoch 1 Training:   0%|          | 18/7890 [00:03<22:42,  5.78it/s]Epoch 1 Training:   0%|          | 19/7890 [00:03<22:43,  5.77it/s]Epoch 1 Training:   0%|          | 20/7890 [00:03<22:37,  5.80it/s]Epoch 1 Training:   0%|          | 21/7890 [00:03<22:36,  5.80it/s]Epoch 1 Training:   0%|          | 22/7890 [00:03<22:51,  5.74it/s]Epoch 1 Training:   0%|          | 23/7890 [00:03<22:48,  5.75it/s]Epoch 1 Training:   0%|          | 24/7890 [00:04<22:51,  5.73it/s]Epoch 1 Training:   0%|          | 25/7890 [00:04<23:29,  5.58it/s]Epoch 1 Training:   0%|          | 26/7890 [00:04<23:19,  5.62it/s]Epoch 1 Training:   0%|          | 27/7890 [00:04<23:07,  5.67it/s]Epoch 1 Training:   0%|          | 28/7890 [00:04<23:02,  5.69it/s]Epoch 1 Training:   0%|          | 29/7890 [00:05<22:46,  5.75it/s]Epoch 1 Training:   0%|          | 30/7890 [00:05<22:37,  5.79it/s]Epoch 1 Training:   0%|          | 31/7890 [00:05<23:09,  5.66it/s]Epoch 1 Training:   0%|          | 32/7890 [00:05<23:02,  5.68it/s]Epoch 1 Training:   0%|          | 33/7890 [00:05<22:49,  5.74it/s]Epoch 1 Training:   0%|          | 34/7890 [00:05<23:01,  5.69it/s]Epoch 1 Training:   0%|          | 35/7890 [00:06<22:53,  5.72it/s]Epoch 1 Training:   0%|          | 36/7890 [00:06<22:44,  5.75it/s]Epoch 1 Training:   0%|          | 37/7890 [00:06<22:47,  5.74it/s]Epoch 1 Training:   0%|          | 38/7890 [00:06<22:43,  5.76it/s]Epoch 1 Training:   0%|          | 39/7890 [00:06<22:44,  5.75it/s]Epoch 1 Training:   1%|          | 40/7890 [00:06<22:42,  5.76it/s]Epoch 1 Training:   1%|          | 41/7890 [00:07<23:13,  5.63it/s]Epoch 1 Training:   1%|          | 42/7890 [00:07<22:59,  5.69it/s]Epoch 1 Training:   1%|          | 43/7890 [00:07<22:32,  5.80it/s]Epoch 1 Training:   1%|          | 44/7890 [00:07<22:29,  5.81it/s]Epoch 1 Training:   1%|          | 45/7890 [00:07<22:38,  5.78it/s]Epoch 1 Training:   1%|          | 46/7890 [00:07<22:54,  5.71it/s]Epoch 1 Training:   1%|          | 47/7890 [00:08<23:13,  5.63it/s]Epoch 1 Training:   1%|          | 48/7890 [00:08<22:57,  5.69it/s]Epoch 1 Training:   1%|          | 49/7890 [00:08<22:44,  5.75it/s]Epoch 1 Training:   1%|          | 50/7890 [00:08<23:07,  5.65it/s]Epoch 1 Training:   1%|          | 51/7890 [00:08<23:26,  5.57it/s]Epoch 1 Training:   1%|          | 52/7890 [00:09<23:10,  5.64it/s]Epoch 1 Training:   1%|          | 53/7890 [00:09<22:52,  5.71it/s]Epoch 1 Training:   1%|          | 54/7890 [00:09<22:55,  5.70it/s]Epoch 1 Training:   1%|          | 55/7890 [00:09<22:47,  5.73it/s]Epoch 1 Training:   1%|          | 57/7890 [00:09<20:03,  6.51it/s]Epoch 1 Training:   1%|          | 58/7890 [00:09<20:23,  6.40it/s]Epoch 1 Training:   1%|          | 59/7890 [00:10<21:04,  6.19it/s]Epoch 1 Training:   1%|          | 60/7890 [00:10<21:17,  6.13it/s]Epoch 1 Training:   1%|          | 61/7890 [00:10<21:44,  6.00it/s]Epoch 1 Training:   1%|          | 62/7890 [00:10<22:00,  5.93it/s]Epoch 1 Training:   1%|          | 63/7890 [00:10<22:26,  5.81it/s]Epoch 1 Training:   1%|          | 64/7890 [00:11<22:36,  5.77it/s]Epoch 1 Training:   1%|          | 65/7890 [00:11<22:34,  5.78it/s]Epoch 1 Training:   1%|          | 66/7890 [00:11<21:00,  6.21it/s]Epoch 1 Training:   1%|          | 67/7890 [00:11<21:08,  6.17it/s]Epoch 1 Training:   1%|          | 68/7890 [00:11<21:38,  6.03it/s]Epoch 1 Training:   1%|          | 69/7890 [00:11<21:57,  5.93it/s]Epoch 1 Training:   1%|          | 70/7890 [00:12<22:50,  5.71it/s]Epoch 1 Training:   1%|          | 71/7890 [00:12<22:46,  5.72it/s]Epoch 1 Training:   1%|          | 72/7890 [00:12<22:43,  5.74it/s]Epoch 1 Training:   1%|          | 73/7890 [00:12<22:45,  5.72it/s]Epoch 1 Training:   1%|          | 74/7890 [00:12<23:23,  5.57it/s]Epoch 1 Training:   1%|          | 75/7890 [00:12<23:13,  5.61it/s]Epoch 1 Training:   1%|          | 76/7890 [00:13<22:54,  5.69it/s]Epoch 1 Training:   1%|          | 77/7890 [00:13<22:43,  5.73it/s]Epoch 1 Training:   1%|          | 79/7890 [00:13<19:52,  6.55it/s]Epoch 1 Training:   1%|          | 80/7890 [00:13<20:33,  6.33it/s]Epoch 1 Training:   1%|          | 80/7890 [00:13<22:23,  5.81it/s]
hello
hello
hello
hello
hello
hello
hello
hello
hello
hello
hello
hello
hello
hello
hello
hello
hello
hello
hello
hello
hello
hello
hello
hello
hello
hello
hello
hello
hello
hello
hello
hello
hello
hello
hello
hello
hello
hello
hello
hello
hello
hello
hello
hello
hello
hello
hello
hello
hello
hello
hello
hello
hello
hello
hello
hello
hello
hello
hello
hello
hello
hello
hello
hello
hello
hello
hello
hello
hello
hello
hello
hello
hello
hello
hello
hello
hello
hello
hello
hello
Traceback (most recent call last):
  File "/data/hlr/chenzheng/data/domiknows_project/DomiKnowS/examples/WIQA/WIQA_aug.py", line 241, in <module>
    program.train(reader_train_aug, train_epoch_num=1, Optim=lambda param: AdamW(param, lr = args.learning_rate,eps = 1e-8 ), device=cur_device)
  File "/data/hlr/chenzheng/data/domiknows_project/DomiKnowS/examples/WIQA/../../regr/program/program.py", line 270, in train
    self.call_epoch('Training', training_set, self.train_epoch, **kwargs)
  File "/data/hlr/chenzheng/data/domiknows_project/DomiKnowS/examples/WIQA/../../regr/program/program.py", line 209, in call_epoch
    consume(tqdm(epoch_fn(dataset, **kwargs), total=get_len(dataset), desc=desc))
  File "/data/hlr/chenzheng/data/domiknows_project/DomiKnowS/examples/WIQA/../../regr/utils.py", line 299, in consume
    collections.deque(it, maxlen=0)
  File "/home/zhengchen/anaconda/anaconda3/envs/domi/lib/python3.9/site-packages/tqdm/std.py", line 1185, in __iter__
    for obj in iterable:
  File "/data/hlr/chenzheng/data/domiknows_project/DomiKnowS/examples/WIQA/../../regr/program/program.py", line 286, in train_epoch
    loss, metric, *output = self.model(data_item)
  File "/home/zhengchen/anaconda/anaconda3/envs/domi/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/data/hlr/chenzheng/data/domiknows_project/DomiKnowS/examples/WIQA/../../regr/program/model/pytorch.py", line 95, in forward
    *out, = self.populate(builder)
  File "/data/hlr/chenzheng/data/domiknows_project/DomiKnowS/examples/WIQA/../../regr/program/model/pytorch.py", line 246, in populate
    data_item = self.inference(builder)
  File "/data/hlr/chenzheng/data/domiknows_project/DomiKnowS/examples/WIQA/../../regr/program/model/pytorch.py", line 235, in inference
    {
  File "/data/hlr/chenzheng/data/domiknows_project/DomiKnowS/examples/WIQA/../../regr/program/model/pytorch.py", line 236, in <lambda>
    'ILP': lambda :datanode.inferILPResults(*self.inference_with, fun=None, epsilon=None),
  File "/data/hlr/chenzheng/data/domiknows_project/DomiKnowS/examples/WIQA/../../regr/graph/dataNode.py", line 1111, in inferILPResults
    myilpOntSolver.calculateILPSelection(self, *conceptsRelations, fun=fun, epsilon = epsilon, minimizeObjective = minimizeObjective, ignorePinLCs = ignorePinLCs)    
  File "/data/hlr/chenzheng/data/domiknows_project/DomiKnowS/examples/WIQA/../../regr/solver/gurobiILPOntSolver.py", line 1002, in calculateILPSelection
    self.addLogicalConstrains(mP, dn, lcs, p)
  File "/data/hlr/chenzheng/data/domiknows_project/DomiKnowS/examples/WIQA/../../regr/solver/gurobiILPOntSolver.py", line 584, in addLogicalConstrains
    result = self.__constructLogicalConstrains(lc, self.myIlpBooleanProcessor, m, dn, p, key = key,  lcVariablesDns = {}, headLC = True)
  File "/data/hlr/chenzheng/data/domiknows_project/DomiKnowS/examples/WIQA/../../regr/solver/gurobiILPOntSolver.py", line 857, in __constructLogicalConstrains
    vDns = self.__constructLogicalConstrains(e, booleanProcesor, m, dn, p, key = key, lcVariablesDns = lcVariablesDns, headLC = False, loss = loss, sample = sample)
  File "/data/hlr/chenzheng/data/domiknows_project/DomiKnowS/examples/WIQA/../../regr/solver/gurobiILPOntSolver.py", line 880, in __constructLogicalConstrains
    return lc(m, booleanProcesor, lcVariables, headConstrain = headLC)
  File "/data/hlr/chenzheng/data/domiknows_project/DomiKnowS/examples/WIQA/../../regr/graph/logicalConstrain.py", line 334, in __call__
    return self.createILPConstrains('And', myIlpBooleanProcessor.andVar, model, v, headConstrain)        
  File "/data/hlr/chenzheng/data/domiknows_project/DomiKnowS/examples/WIQA/../../regr/graph/logicalConstrain.py", line 270, in createILPConstrains
    tVars.append(lcFun(model, *t, onlyConstrains = headConstrain))
  File "/data/hlr/chenzheng/data/domiknows_project/DomiKnowS/examples/WIQA/../../regr/solver/gurobiILPBooleanMethods.py", line 313, in andVar
    if self.ifLog: self.myLogger.warning("%s ignoring %f - incorrect"%(logicMethodName,currentVar)) 
TypeError: must be real number, not list
