{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "special-somalia",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root Folder Absoloute path:  /home/hfaghihi/Framework/fix/DomiKnowS\n"
     ]
    }
   ],
   "source": [
    "import os,sys,inspect\n",
    "currentdir = os.path.dirname(os.path.abspath(inspect.getfile(inspect.currentframe())))\n",
    "parent_dir = os.path.abspath(os.path.join(currentdir, os.pardir))\n",
    "root = os.path.dirname(parent_dir)\n",
    "print(\"root Folder Absoloute path: \", root)\n",
    "\n",
    "import sys\n",
    "sys.path.append(root)\n",
    "\n",
    "import logging\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "from IPython.display import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "narrow-source",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "391\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import torch\n",
    "\n",
    "with open(\"data/train_propara_roberta_version_trips.json\", \"r\") as file:\n",
    "    data = json.load(file)\n",
    "print(len(data))\n",
    "updated_data = []\n",
    "lines = 0\n",
    "for item in data:\n",
    "    lines += len(item[\"sentence_texts\"])\n",
    "    try:\n",
    "        temp = item.copy()\n",
    "        new_state = []\n",
    "        location_types = []\n",
    "        for state_id, states in enumerate(temp['states']):\n",
    "            temp2 = [\"-\", ]\n",
    "            \n",
    "            ltemp = []\n",
    "            if states[0] == \"?\":\n",
    "                ltemp.append(\"Unknown\")\n",
    "            elif states[0] == \"-\":\n",
    "                ltemp.append(\"-\")\n",
    "            else:\n",
    "                ltemp.append(\"Known\")\n",
    "                \n",
    "            prev_loc = states[0]\n",
    "            for loc in states[1:]:\n",
    "                if loc == \"?\":\n",
    "                    ltemp.append(\"Unknown\")\n",
    "                elif loc == \"-\":\n",
    "                    ltemp.append(\"-\")\n",
    "                else:\n",
    "                    ltemp.append(\"Known\")\n",
    "\n",
    "                if loc == prev_loc:\n",
    "                    temp2.append(\"No change\")\n",
    "                elif loc != \"-\" and prev_loc == \"-\":\n",
    "                    temp2.append(\"Create\")\n",
    "                elif loc == \"-\" and prev_loc != \"-\":\n",
    "                    temp2.append(\"Destroy\")\n",
    "                else:\n",
    "                    temp2.append(\"Move\")\n",
    "                prev_loc = loc\n",
    "            new_state.append(temp2)\n",
    "            location_types.append(ltemp)\n",
    "        temp['actions'] = new_state\n",
    "        temp['ltype'] = location_types\n",
    "        \n",
    "\n",
    "#         new_annotation = []\n",
    "#         for state in temp['states_annotation']:\n",
    "#             temp2 = state\n",
    "#             for loc in temp2:\n",
    "#                 if loc[0] == \"?\":\n",
    "#                     loc[2] = item['boundaries'][-1][1] + 1\n",
    "#                     loc[3] = item['boundaries'][-1][1] + 1\n",
    "#             new_annotation.append(temp2)\n",
    "\n",
    "#         temp['new_annotation'] = new_annotation\n",
    "    except:\n",
    "        print(item)\n",
    "        raise\n",
    "    updated_data.append(temp)\n",
    "    \n",
    "for item in updated_data:\n",
    "    steps = len(item['sentence_paragraph'])\n",
    "    entities = len(item['participants'])\n",
    "    probs = torch.softmax(torch.randn(steps, entities, 4), dim=-1)\n",
    "    item['action_probs'] = probs.tolist()\n",
    "    tprobs = torch.softmax(torch.randn(steps, entities, 4), dim=-1)\n",
    "    item['Taction_probs'] = tprobs.tolist()\n",
    "    \n",
    "with open(\"data/train.json\", \"w\") as file:\n",
    "    json.dump(updated_data, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "referenced-press",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "391\n"
     ]
    }
   ],
   "source": [
    "with open(\"data/train.json\", \"r\") as file:\n",
    "    data = json.load(file)\n",
    "print(len(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "prescription-flooring",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from regr.data.reader import RegrReader\n",
    "import torch\n",
    "\n",
    "\n",
    "class ProparaReader(RegrReader):\n",
    "    def getProcedureIDval(self, item):\n",
    "        return [item[\"para_id\"]]\n",
    "\n",
    "    def getEntitiesval(self, item):\n",
    "        return ' '.join(item['participants'])\n",
    "        \n",
    "    def getEntityval(self, item):\n",
    "        return item[\"participants\"]\n",
    "    \n",
    "    def getContextval(self, item):\n",
    "        return item['sentence_paragraph']\n",
    "\n",
    "    def getStepval(self, item):\n",
    "        sentences = item[\"sentence_texts\"]\n",
    "        return  sentences\n",
    "    \n",
    "    def getActionval(self, item):\n",
    "        actions = []\n",
    "        for step, step_text in enumerate(item['sentence_texts']):\n",
    "            actions.append([])\n",
    "            for eid, entity in enumerate(item['participants']):\n",
    "                actions[-1].append(item['action_probs'][eid])\n",
    "\n",
    "        return torch.tensor(actions)\n",
    "    \n",
    "    def getTActionval(self, item):\n",
    "        actions = []\n",
    "        for step, step_text in enumerate(item['sentence_texts']):\n",
    "            actions.append([])\n",
    "            for eid, entity in enumerate(item['participants']):\n",
    "                actions[-1].append(item['Taction_probs'])\n",
    "\n",
    "        return torch.tensor(actions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "collectible-juice",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_reader = ProparaReader(\"data/train.json\")\n",
    "# next(iter(train_reader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "external-possible",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:23: UserWarning: Please use OrderedDict rather than dict to prevent unpredictable order of arguments.For this instance, OrderedDict([('arg1', 'step'), ('arg2', 'step')]) is used.\n"
     ]
    }
   ],
   "source": [
    "from regr.graph import Graph, Concept, Relation\n",
    "from regr.graph.logicalConstrain import orL, andL, existsL, notL, atLeastL, atMostL, ifL, nandL, V, exactL\n",
    "\n",
    "Graph.clear()\n",
    "Concept.clear()\n",
    "Relation.clear()\n",
    "\n",
    "with Graph('global') as graph:\n",
    "    procedure = Concept(\"procedure\")\n",
    "    \n",
    "    context = Concept(\"context\")\n",
    "    entities = Concept(\"entities\")\n",
    "    \n",
    "    (procedure_context, procedure_entities) = procedure.has_a(context, entities)\n",
    "    \n",
    "    entity = Concept('entity')\n",
    "    (entity_rel, ) = entities.contains(entity)\n",
    "    \n",
    "    step = Concept(\"step\")\n",
    "    (context_step, ) = context.contains(step)\n",
    "    \n",
    "    before = Concept(\"before\")\n",
    "    (before_arg1, before_arg2) = before.has_a(arg1=step, arg2=step)\n",
    "    \n",
    "    action = Concept(name='action')\n",
    "    (action_step, action_entity) = action.has_a(step, entity)\n",
    "    \n",
    "    create = action(name=\"create\")\n",
    "    destroy = action(name=\"destroy\")\n",
    "    move = action(name=\"move\")\n",
    "    nochange = action('none')\n",
    "    \n",
    "    Tcreate = action(name=\"trips_create\")\n",
    "    Tdestroy = action(name=\"trips_destroy\")\n",
    "    Tmove = action(name=\"trips_move\")\n",
    "    Tnochange = action('trips_none')\n",
    "\n",
    "    exactL(Tcreate, Tdestroy, Tmove, Tnochange)\n",
    "    exactL(create, destroy, move, nochange)\n",
    "    \n",
    "    ifL(Tcreate, create)\n",
    "    ifL(Tdestroy, destroy)\n",
    "    ifL(Tmove, move)\n",
    "    ifL(Tnochange, nochange)\n",
    "    \n",
    "#     ifL(\n",
    "#         destroy('x'), \n",
    "#         notL(\n",
    "#             andL(\n",
    "#                 existsL(\n",
    "#                     andL(destroy('y'), existsL(before('y', path=('x', arg1, 'before', arg1)))),\n",
    "#                     notL(\n",
    "#                         existsL(\n",
    "#                             andL(\n",
    "#                                 create('z'), \n",
    "#                                 eqL(before('z'))\n",
    "#                             )\n",
    "#                         )\n",
    "#                     )\n",
    "#                 )\n",
    "#             )\n",
    "#         )\n",
    "#     )\n",
    "#   atMostL(1, (\"x\"), andL(entity, \"y\", create()))\n",
    "    # No entity_step\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "decent-illinois",
   "metadata": {},
   "outputs": [],
   "source": [
    "from regr.sensor.pytorch.sensors import FunctionalSensor, JointSensor, ReaderSensor, FunctionalReaderSensor, JointFunctionalReaderSensor\n",
    "from regr.sensor.pytorch.learners import ModuleLearner\n",
    "from regr.sensor.pytorch.relation_sensors import CompositionCandidateSensor\n",
    "from regr.sensor.pytorch.query_sensor import DataNodeReaderSensor\n",
    "\n",
    "procedure['id'] = ReaderSensor(keyword='ProcedureID')\n",
    "context['text'] = ReaderSensor(keyword='Context')\n",
    "entities['text'] = ReaderSensor(keyword='Entities')\n",
    "\n",
    "def make_procedure(arg1m, arg2m, data):\n",
    "    total_procedures = len(arg1m) * len(arg2m)\n",
    "    rel_links1 = torch.zeros(total_procedures, len(arg1m))\n",
    "    rel_links2 = torch.zeros(total_procedures, len(arg2m))\n",
    "    past1 = 0\n",
    "    past2 = 0\n",
    "    for i in range(total_procedures):\n",
    "        rel_links1[i, past2: past2 + len(arg2m)] = 1\n",
    "        past2 = past2 + len(arg2m)\n",
    "        rel_links2[i, past1: past1 + len(arg1m)] = 1\n",
    "        past1 = past1 + len(arg1m)\n",
    "    \n",
    "    return rel_links1, rel_links2, data\n",
    "    \n",
    "    \n",
    "procedure[procedure_context.reversed, procedure_entities.reversed, \"id\"] = JointFunctionalReaderSensor(context['text'], entities['text'], keyword=\"ProcedureID\", forward=make_procedure)\n",
    "\n",
    "def read_initials(*prev, data):\n",
    "    number = len(data)\n",
    "    rel_links = torch.ones(number, 1)\n",
    "    indexes = [i for i in range(number)]\n",
    "    return rel_links, data, indexes\n",
    "\n",
    "entity[entity_rel, 'text', 'index'] = JointFunctionalReaderSensor(entities['text'], keyword='Entity', forward=read_initials)\n",
    "\n",
    "step[context_step, 'text', 'index'] = JointFunctionalReaderSensor(context['text'], keyword='Step', forward=read_initials)\n",
    "\n",
    "def make_actions(steps, entities):\n",
    "    all_actions = len(steps) * len(entities)\n",
    "    link1 = torch.zeros(all_actions, len(steps))\n",
    "    link2 = torch.zeros(all_actions, len(entities))\n",
    "    for i in range(all_actions):\n",
    "        link1[i: steps[i] * len(entities): (steps[i] + 1) * len(entities)] = 1\n",
    "        link2[i: entities[i] * len(steps): (entities[i] + 1) * len(steps)] = 1\n",
    "    \n",
    "    return link1, link2\n",
    "    \n",
    "action[action_step.reversed, action_entity.reversed] = FunctionalSensor(step['index'], entity['index'], forward=make_actions)\n",
    "\n",
    "# def read_labels(arg1, arg2, data):\n",
    "    \n",
    "# action[nochange, destroy, create, move] = FunctionalReaderSensor(action_step.reversed, action_entity.reversed, keyword=\"Action\", forward=read_labels)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
