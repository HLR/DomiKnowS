{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/hfaghihi/Framework/DomiKnowS/examples/ACE05\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "print(os.getcwd())\n",
    "# Please change the root to an absolute or relative path to DomiKnowS root.\n",
    "# In case relative path is used, consider the printed `CWD` as current working directory.\n",
    "root = '/home/hfaghihi/Framework/DomiKnowS'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(root)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any\n",
    "from regr.sensor.pytorch.sensors import ReaderSensor, ConstantSensor, FunctionalSensor, FunctionalReaderSensor, TorchEdgeSensor\n",
    "\n",
    "\n",
    "class MultiLevelReaderSensor(ConstantSensor):\n",
    "    def __init__(self, *pres, keyword=None, edges=None, label=False, device='auto'):\n",
    "        super().__init__(*pres, data=None, edges=edges, label=label, device=device)\n",
    "        self.keyword = keyword\n",
    "\n",
    "    def fill_data(self, data_item):\n",
    "        try:\n",
    "            if isinstance(self.keyword, tuple):\n",
    "                self.data = (self.fetch_key(data_item, keyword) for keyword in self.keyword)\n",
    "            else:\n",
    "                self.data = self.fetch_key(data_item, self.keyword)\n",
    "        except KeyError as e:\n",
    "            raise KeyError(\"The key you requested from the reader doesn't exist: %s\" % str(e))\n",
    "\n",
    "    def fetch_key(self, data_item, key):\n",
    "        data = []\n",
    "        if \".\" in key:\n",
    "            keys = key.split(\".\")\n",
    "            items = data_item\n",
    "            loop = 0\n",
    "            direct_loop = True\n",
    "            for key in keys:\n",
    "                if key == \"*\":\n",
    "                    loop += 1\n",
    "                    if loop == 1:\n",
    "                        keys = items.keys()\n",
    "                        items = [items[key] for key in keys]\n",
    "                    if loop > 1:\n",
    "                        keys = [item.keys() for item in items]\n",
    "                        new_items = []\n",
    "                        for index, item in enumerate(items):\n",
    "                            for index1, key in enumerate(keys[index]):\n",
    "                                new_items.append(item[key])\n",
    "                        items = new_items\n",
    "                else:\n",
    "                    if loop == 0:\n",
    "                        items = items[key]\n",
    "                    if loop > 0:\n",
    "                        items = [it[key] for it in items]\n",
    "            data = items\n",
    "        else:\n",
    "            data = data_item[key]\n",
    "\n",
    "        return data\n",
    "        \n",
    "        \n",
    "    def forward(self, *_) -> Any:\n",
    "        if isinstance(self.keyword, tuple) and isinstance(self.data, tuple):\n",
    "            return (super().forward(data) for data in self.data)\n",
    "        else:\n",
    "            return super().forward(self.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "from regr.graph import Concept\n",
    "from regr.program import POIProgram\n",
    "from regr.sensor.pytorch.sensors import ReaderSensor, ConstantSensor, FunctionalSensor, FunctionalReaderSensor, TorchEdgeSensor\n",
    "from regr.sensor.pytorch.learners import ModuleLearner\n",
    "from regr.sensor.pytorch.relation_sensors import CandidateSensor, CandidateRelationSensor\n",
    "\n",
    "from sensors.tokenizers import TokenizerEdgeSensor\n",
    "from models import Tokenizer, BERT, SpanClassifier, cartesian_concat, token_to_span_candidate, span_candidate_emb, span_label, span_emb, find_is_a\n",
    "\n",
    "\n",
    "def model(graph):\n",
    "    graph.detach()\n",
    "\n",
    "    ling_graph = graph['linguistic']\n",
    "    ace05_graph = graph['ACE05']\n",
    "    entities_graph = ace05_graph['Entities']\n",
    "    relations_graph = ace05_graph['Relations']\n",
    "    events_graph = ace05_graph['Events']\n",
    "\n",
    "    document = ling_graph['document']\n",
    "    token = ling_graph['token']\n",
    "    span_candidate = ling_graph['span_candidate']\n",
    "    span_annotation = ling_graph['span_annotation']\n",
    "    span = ling_graph['span']\n",
    "    document_contains_token = document.relate_to(token)[0]\n",
    "    span_contains_token = span.relate_to(token)[0]\n",
    "    span_is_span_candidate = span.relate_to(span_candidate)[0]\n",
    "\n",
    "    document['index'] = ReaderSensor(keyword='text')\n",
    "    document_contains_token['forward'] = TokenizerEdgeSensor('index', mode='forward', to=('index', 'ids', 'offset'), tokenizer=Tokenizer())\n",
    "    token['emb'] = ModuleLearner('ids', module=BERT())\n",
    "\n",
    "    span_annotation['extent'] = MultiLevelReaderSensor(keyword=\"spans.*.mentions.*.extent.start\")\n",
    "    span_annotation['extent'] = ConstantSensor(data=[\"a\", \"b\", \"c\", \"d\"])\n",
    "    span_annotation['type'] = MultiLevelReaderSensor(keyword=\"spans.*.mentions.*.type\")\n",
    "\n",
    "    program = POIProgram(graph, poi=(token, span_candidate, span,))\n",
    "\n",
    "    return program\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ace05.reader import Reader, DictReader\n",
    "import config\n",
    "\n",
    "sensor = MultiLevelReaderSensor(keyword=\"spans.*.mentions.*.head.start\")\n",
    "sensor1 = MultiLevelReaderSensor(keyword=\"spans.*.mentions.*.type\")\n",
    "traint_reader = DictReader(config.path, list_path=config.list_path, type='train', status=config.status)\n",
    "# sensor.fill_data(next(iter(traint_reader)))\n",
    "# sensor1.fill_data(next(iter(traint_reader)))\n",
    "# print(sensor.data)\n",
    "# print(sensor1.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'NOM'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "item = next(iter(traint_reader))\n",
    "item['spans']['MARKETVIEW_20050228.2211-E1']['mentions']['MARKETVIEW_20050228.2211-E1-10']['type']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from regr.sensor.pytorch.query_sensor import DataNodeSensor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_key(data_item, key):\n",
    "        data = []\n",
    "        if \".\" in key:\n",
    "            keys = key.split(\".\")\n",
    "            items = data_item\n",
    "            loop = 0\n",
    "            direct_loop = True\n",
    "            for key in keys:\n",
    "                if key == \"*\":\n",
    "                    loop += 1\n",
    "                    if loop == 1:\n",
    "                        keys = items.keys()\n",
    "                        items = [items[key] for key in keys]\n",
    "                    if loop > 1:\n",
    "                        keys = [item.keys() for item in items]\n",
    "                        new_items = []\n",
    "                        for index, item in enumerate(items):\n",
    "                            for index1, key in enumerate(keys[index]):\n",
    "                                new_items.append(item[key])\n",
    "                        items = new_items\n",
    "                else:\n",
    "                    if key == \"subtype\":\n",
    "                        new_items = []\n",
    "                        for item in items:\n",
    "                            for i in range(len(item['mentions'])):\n",
    "                                if  isinstance(item[key], Concept):\n",
    "                                    new_items.append(item[key].name)\n",
    "                                else:\n",
    "                                    new_items.append(None)\n",
    "                        items = new_items\n",
    "                        \n",
    "                    elif key == \"type\":\n",
    "                        new_items = []\n",
    "                        for item in items:\n",
    "                            for i in range(len(item['mentions'])):\n",
    "                                if  isinstance(item[key], Concept):\n",
    "                                    new_items.append(item[key].name)\n",
    "                                else:\n",
    "                                    new_items.append(None)\n",
    "                        items = new_items\n",
    "                    elif loop == 0:\n",
    "                        items = items[key]\n",
    "                    elif loop > 0:\n",
    "                        items = [it[key] for it in items]\n",
    "                    \n",
    "            data = items\n",
    "        else:\n",
    "            data = data_item[key]\n",
    "        \n",
    "        return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ORG', 'ORG', 'ORG', 'PER', 'PER', 'PER', 'PER', 'PER', 'PER', 'PER', 'PER', 'PER', 'PER', 'PER', 'PER', 'ORG', 'ORG', 'ORG', 'ORG', 'ORG', 'ORG', 'ORG', 'ORG', 'PER', 'PER', 'PER', 'PER', 'PER', 'PER', 'Timex2', 'Timex2']\n",
      "31\n"
     ]
    }
   ],
   "source": [
    "input_t = fetch_key(item, \"spans.*.type\")\n",
    "print(input_t)\n",
    "print(len(input_t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "traint_reader = DictReader(config.path, list_path=config.list_path, type='train', status=config.status)\n",
    "item = next(iter(traint_reader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'start': 281, 'end': 286, 'text': 'offer'}"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from ace05.annotation import *\n",
    "item['events']['MARKETVIEW_20050228.2211-EV1']['mentions']['MARKETVIEW_20050228.2211-EV1-1']['anchor']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[281, 738, 775, 304, 412, 640, 824]\n"
     ]
    }
   ],
   "source": [
    "sensor1 = MultiLevelReaderSensor(keyword=\"events.*.mentions.*.anchor.start\")\n",
    "traint_reader = DictReader(config.path, list_path=config.list_path, type='train', status=config.status)\n",
    "sensor1.fill_data(next(iter(traint_reader)))\n",
    "print(sensor1.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "class CustomConstantSensor(ConstantSensor):\n",
    "    def __init__(self, *pres, keyword=None, edges=None, label=False, device='auto', concept=None, data=None):\n",
    "        super().__init__(*pres, data=data, edges=edges, label=label, device=device)\n",
    "        self.concept_name = concept\n",
    "        \n",
    "    def forward(self, *_) -> Any:\n",
    "        output = []\n",
    "        for data in self.data:\n",
    "            if data == self.concept_name:\n",
    "                output.append(1)\n",
    "            else:\n",
    "                output.append(0)\n",
    "        if self.label == True:\n",
    "            output = torch.tensor(output)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "sensor_c = CustomConstantSensor(label=True, concept='PER-Individual', data = input_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1,\n",
       "        1, 0, 0, 0, 1, 0, 0])"
      ]
     },
     "execution_count": 175,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sensor_c.forward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}