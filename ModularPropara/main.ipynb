{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "external-artwork",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/hfaghihi/Framework/fix/DomiKnowS/examples\n",
      "root Folder Absoloute path:  /home/hfaghihi/Framework/fix/DomiKnowS\n"
     ]
    }
   ],
   "source": [
    "import os,sys,inspect\n",
    "import json\n",
    "import torch\n",
    "currentdir = os.path.dirname(os.getcwd())\n",
    "print(currentdir)\n",
    "# parent_dir = os.path.abspath(os.path.join(currentdir, os.pardir))\n",
    "root = os.path.dirname(currentdir)\n",
    "print(\"root Folder Absoloute path: \", root)\n",
    "\n",
    "\n",
    "import sys\n",
    "sys.path.append(root)\n",
    "\n",
    "import logging\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "from IPython.display import Image\n",
    "\n",
    "import __main__\n",
    "\n",
    "__main__.__file__=\"main.py\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "understanding-feelings",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "54\n",
      "dict_keys(['para_id', 'location_cand', 'states', 'loc_spans', 'boundaries', 'states_mapping', 'states_in_tokens', 'spans', 'roberta_tokens', 'states_annotation', 'roberta_bounds', 'participants', 'sentence_paragraph', 'sentence_texts', 'trips_annotation', 'actions', 'ltype', 'location_entity_cands', 'verb_mentions', 'entity_mentions', 'computed_boundaries', 'tags', 'location_indexes', 'net_results'])\n"
     ]
    }
   ],
   "source": [
    "with open(\"data/tracker.log\", \"r\") as file:\n",
    "    tracker = []\n",
    "    for line in file:\n",
    "        tracker.append(json.loads(line))\n",
    "print(len(tracker))\n",
    "print(tracker[0].keys())\n",
    "state2idx = {'O_C': 0, 'O_D': 1, 'E': 2, 'M': 3, 'C': 4, 'D': 5}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "chinese-narrative",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 5, 6])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "torch.tensor(tracker[10]['net_results']['tag_logits']).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "hourly-favorite",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "391\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import torch\n",
    "\n",
    "with open(\"data/train_propara_roberta_version_trips.json\", \"r\") as file:\n",
    "    data = json.load(file)\n",
    "print(len(data))\n",
    "updated_data = []\n",
    "lines = 0\n",
    "for item in tracker:\n",
    "    lines += len(item[\"sentence_texts\"])\n",
    "    try:\n",
    "        temp = item.copy()\n",
    "        new_state = []\n",
    "        location_types = []\n",
    "        for state_id, states in enumerate(temp['states']):\n",
    "            temp2 = [\"-\", ]\n",
    "            \n",
    "            ltemp = []\n",
    "            if states[0] == \"?\":\n",
    "                ltemp.append(\"Unknown\")\n",
    "            elif states[0] == \"-\":\n",
    "                ltemp.append(\"-\")\n",
    "            else:\n",
    "                ltemp.append(\"Known\")\n",
    "                \n",
    "            prev_loc = states[0]\n",
    "            for loc in states[1:]:\n",
    "                if loc == \"?\":\n",
    "                    ltemp.append(\"Unknown\")\n",
    "                elif loc == \"-\":\n",
    "                    ltemp.append(\"-\")\n",
    "                else:\n",
    "                    ltemp.append(\"Known\")\n",
    "\n",
    "                if loc == prev_loc:\n",
    "                    temp2.append(\"No change\")\n",
    "                elif loc != \"-\" and prev_loc == \"-\":\n",
    "                    temp2.append(\"Create\")\n",
    "                elif loc == \"-\" and prev_loc != \"-\":\n",
    "                    temp2.append(\"Destroy\")\n",
    "                else:\n",
    "                    temp2.append(\"Move\")\n",
    "                prev_loc = loc\n",
    "            new_state.append(temp2)\n",
    "            location_types.append(ltemp)\n",
    "        temp['actions'] = new_state\n",
    "        temp['ltype'] = location_types\n",
    "        \n",
    "\n",
    "#         new_annotation = []\n",
    "#         for state in temp['states_annotation']:\n",
    "#             temp2 = state\n",
    "#             for loc in temp2:\n",
    "#                 if loc[0] == \"?\":\n",
    "#                     loc[2] = item['boundaries'][-1][1] + 1\n",
    "#                     loc[3] = item['boundaries'][-1][1] + 1\n",
    "#             new_annotation.append(temp2)\n",
    "\n",
    "#         temp['new_annotation'] = new_annotation\n",
    "    except:\n",
    "        print(item)\n",
    "        raise\n",
    "    updated_data.append(temp)\n",
    "    \n",
    "for item in updated_data:\n",
    "    steps = len(item['sentence_paragraph'])\n",
    "    entities = len(item['participants'])\n",
    "    probs = torch.tensor(item['net_results']['tag_logits']).softmax(dim=-1)\n",
    "    probs[:, :, 0] = probs[:, :, 0] + probs[:, :, 1] + probs[:, :, 2]\n",
    "    probs = torch.cat((probs[:, :, 0:1], probs[:, :, 3:]), dim=-1)\n",
    "    item['action_probs'] = probs.tolist()\n",
    "#     tprobs = torch.softmax(torch.randn(steps, entities, 4), dim=-1)\n",
    "#     item['Taction_probs'] = tprobs.tolist()\n",
    "    \n",
    "with open(\"data/train.json\", \"w\") as file:\n",
    "    json.dump(updated_data, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "exceptional-unknown",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "54\n"
     ]
    }
   ],
   "source": [
    "with open(\"data/train.json\", \"r\") as file:\n",
    "    data = json.load(file)\n",
    "print(len(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "after-manner",
   "metadata": {},
   "outputs": [],
   "source": [
    "from regr.data.reader import RegrReader\n",
    "import torch\n",
    "\n",
    "\n",
    "class ProparaReader(RegrReader):\n",
    "    def getProcedureIDval(self, item):\n",
    "        return [item[\"para_id\"]]\n",
    "\n",
    "    def getEntitiesval(self, item):\n",
    "        return [' '.join(item['participants'])]\n",
    "        \n",
    "    def getEntityval(self, item):\n",
    "        return item[\"participants\"]\n",
    "    \n",
    "    def getContextval(self, item):\n",
    "        return [item['sentence_paragraph']]\n",
    "\n",
    "    def getStepval(self, item):\n",
    "        sentences = item[\"sentence_texts\"]\n",
    "        return  sentences\n",
    "    \n",
    "    def getActionval(self, item):\n",
    "        return torch.tensor(item['action_probs'])\n",
    "    \n",
    "    def getbeforeval(self, item):\n",
    "        b1s = []\n",
    "        b2s = []\n",
    "#         for step in range(len(item[\"sentence_texts\"])):\n",
    "#             b1 = torch.zeros(len(item[\"sentence_texts\"]))\n",
    "#             b1[step] = 1\n",
    "#             for step1 in range(len(item[\"sentence_texts\"])):\n",
    "#                 b2 = torch.zeros(len(item[\"sentence_texts\"]))\n",
    "#                 b2[step1] = 1\n",
    "#                 b1s.append(b1)\n",
    "#                 b2s.append(b2)\n",
    "#         return torch.stack(b1s), torch.stack(b2s)\n",
    "        for step in range(len(item['sentence_texts'])):\n",
    "            for step1 in range(len(item['sentence_texts'])):\n",
    "                if step < step1:\n",
    "                    b1 = torch.zeros(len(item[\"sentence_texts\"]))\n",
    "                    b1[step] = 1\n",
    "                    b2 = torch.zeros(len(item[\"sentence_texts\"]))\n",
    "                    b2[step1] = 1\n",
    "                    b1s.append(b1)\n",
    "                    b2s.append(b2)\n",
    "                    \n",
    "                    \n",
    "        return torch.stack(b1s), torch.stack(b2s)\n",
    "        \n",
    "\n",
    "    def getbefore_trueval(self, item):\n",
    "        num_steps = len(item[\"sentence_texts\"])\n",
    "        values = torch.zeros(num_steps * num_steps)\n",
    "        for step in range(len(item[\"sentence_texts\"])):\n",
    "            for step1 in range(step + 1, len(item[\"sentence_texts\"])):\n",
    "                values[(step * num_steps) + step1] = 1\n",
    "        return values\n",
    "    \n",
    "    def getexact_beforeval(self, item):\n",
    "        b1s = []\n",
    "        b2s = []\n",
    "        for step in range(len(item['sentence_texts'])):\n",
    "            step1 = step + 1\n",
    "            if step1 < len(item['sentence_texts']):\n",
    "                b1 = torch.zeros(len(item[\"sentence_texts\"]))\n",
    "                b1[step] = 1\n",
    "                b2 = torch.zeros(len(item[\"sentence_texts\"]))\n",
    "                b2[step1] = 1\n",
    "                b1s.append(b1)\n",
    "                b2s.append(b2)\n",
    "                    \n",
    "                    \n",
    "        return torch.stack(b1s), torch.stack(b2s)\n",
    "    \n",
    "    def getLocationsval(self, item):\n",
    "        lc = item['location_cand'].copy()\n",
    "        lc.append(\"?\")\n",
    "        return [lc]\n",
    "    \n",
    "    def getLocationval(self, item):\n",
    "        lc = item['location_cand'].copy()\n",
    "        lc.append(\"?\")\n",
    "        return lc\n",
    "    \n",
    "    def getLocationLabelval(self, item):\n",
    "        return torch.tensor(item['net_results']['location'])[:, 1:, :]\n",
    "                \n",
    "    \n",
    "#     def getTActionval(self, item):\n",
    "#         actions = []\n",
    "#         for step, step_text in enumerate(item['sentence_texts']):\n",
    "#             actions.append([])\n",
    "#             for eid, entity in enumerate(item['participants']):\n",
    "#                 actions[-1].append(item['Taction_probs'])\n",
    "\n",
    "#         return torch.tensor(actions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "terminal-welcome",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20\n",
      "6\n",
      "5\n",
      "torch.Size([5, 6, 20])\n"
     ]
    }
   ],
   "source": [
    "train_reader = ProparaReader(\"data/train.json\")\n",
    "runner = iter(train_reader)\n",
    "sample = next(runner)\n",
    "# print(sample['before'][0].shape)\n",
    "# print(sample['before'][1].shape)\n",
    "# print(sample['before'])\n",
    "# print(sample['exact_before'][0].shape)\n",
    "# print(sample['before_true'])\n",
    "print(len(sample['Location']))\n",
    "print(len(sample['Step']))\n",
    "print(len(sample['Entity']))\n",
    "\n",
    "print(sample['LocationLabel'].shape)\n",
    "# select = torch.where(sample['before_true']==1, True, False)\n",
    "# print(select)\n",
    "\n",
    "# print(len(sample['Step']))\n",
    "# print(sample['before'])\n",
    "# print(select.shape)\n",
    "# print(sample['before'][0].shape)\n",
    "# print(torch.masked_select(sample['before'][0], select))\n",
    "# assert sample['before'][0].shape == sample['before'][1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "otherwise-netscape",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "main.py:29: UserWarning: Please use OrderedDict rather than dict to prevent unpredictable order of arguments.For this instance, OrderedDict([('arg1', 'step'), ('arg2', 'step')]) is used.\n",
      "  \n",
      "main.py:32: UserWarning: Please use OrderedDict rather than dict to prevent unpredictable order of arguments.For this instance, OrderedDict([('earg1', 'step'), ('earg2', 'step')]) is used.\n",
      "  \n",
      "main.py:35: UserWarning: Please use OrderedDict rather than dict to prevent unpredictable order of arguments.For this instance, OrderedDict([('step', 'step'), ('entity', 'entity')]) is used.\n",
      "  step[procedure_contain_step, \"text\"] = JoinReaderSensor(procedure[\"id\"], keyword=\"steps\")\n",
      "main.py:38: UserWarning: Please use OrderedDict rather than dict to prevent unpredictable order of arguments.For this instance, OrderedDict([('lentity', 'entity'), ('lstep', 'step'), ('llocation', 'location')]) is used.\n",
      "  \n",
      "main.py:46: UserWarning: Please use OrderedDict rather than dict to prevent unpredictable order of arguments.For this instance, OrderedDict([('se1', 'entity'), ('se2', 'location')]) is used.\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "from regr.graph import Graph, Concept, Relation\n",
    "from regr.graph.logicalConstrain import orL, andL, existsL, notL, atLeastL, atMostL, ifL, nandL, V, exactL\n",
    "from regr.graph import EnumConcept\n",
    "\n",
    "\n",
    "Graph.clear()\n",
    "Concept.clear()\n",
    "Relation.clear()\n",
    "\n",
    "with Graph('global') as graph:\n",
    "    procedure = Concept(name=\"procedure\")\n",
    "    \n",
    "    context = Concept(name=\"context\")\n",
    "    entities = Concept(name=\"entities\")\n",
    "    locations = Concept(name='locations')\n",
    "    \n",
    "    (procedure_context, procedure_entities, procedure_locations) = procedure.has_a(context, entities, locations)\n",
    "    \n",
    "    entity = Concept(name='entity')\n",
    "    (entity_rel, ) = entities.contains(entity)\n",
    "    \n",
    "    step = Concept(name=\"step\")\n",
    "    (context_step, ) = context.contains(step)\n",
    "    \n",
    "    location = Concept(name=\"location\")\n",
    "    (loc_rel, ) = locations.contains(location)\n",
    "    \n",
    "    before = Concept(name=\"before\")\n",
    "    (before_arg1, before_arg2) = before.has_a(arg1=step, arg2=step)\n",
    "    \n",
    "    exact_before = Concept(name=\"exact_before\")\n",
    "    (ebefore_arg1, ebefore_arg2) = exact_before.has_a(earg1=step, earg2=step)\n",
    "    \n",
    "    action = Concept(name='action')\n",
    "    (action_step, action_entity) = action.has_a(step=step, entity=entity)\n",
    "    \n",
    "    entity_location = Concept(name='entity_location')\n",
    "    (lentity, lstep, llocation) = entity_location.has_a(lentity=entity, lstep=step, llocation=location)\n",
    "    \n",
    "    entity_location_label = entity_location(name='entity_location_label')\n",
    "    \n",
    "    \n",
    "    action_label = action(name=\"action_label\", ConceptClass=EnumConcept, values=[\"nochange\", \"move\", \"create\", \"destroy\"])\n",
    "    \n",
    "    same_mention = Concept(name='same_mention')\n",
    "    (same_entity, same_location) = same_mention.has_a(se1=entity, se2=location)\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    #  ------------ Destroy\n",
    "    # No subsequent destroy action unless there is a create action between them\n",
    "    \n",
    "    ifL(\n",
    "        # action a1 is destroy, i is a1's step and e is action entity\n",
    "        andL(\n",
    "            action_label.destroy('a1'), \n",
    "            step('i', path=('a1', action_step)),\n",
    "            entity('e', path=('a1', action_entity))\n",
    "            ), \n",
    "        # then either\n",
    "        orL(\n",
    "            # step j associated with entity e, which is before step i cannot be associated with destroy action a2\n",
    "            ifL(\n",
    "                step('j', path=(('i', before_arg2.reversed, before_arg1))), \n",
    "                notL(action_label.destroy('a2', path=(('j', action_step.reversed), ('e', action_entity.reversed))))\n",
    "                ), \n",
    "            # or if  \n",
    "            ifL(\n",
    "                # step j1 which is before step i is associated with destroy action a2\n",
    "                andL(\n",
    "                    step('j1', path=('i', before_arg2.reversed, before_arg1)), \n",
    "                    action_label.destroy('a3', path=(('j1', action_step.reversed), ('e', action_entity.reversed)))\n",
    "                    ), \n",
    "                # then exists step k associated with entity e, which is between step i and j1 associated with create action a3\n",
    "                existsL(\n",
    "                    andL(\n",
    "                        step('k', path=(('j1', before_arg1.reversed, before_arg2), ('i', before_arg2.reversed, before_arg1))), \n",
    "                        action_label.create('a4', path=(('k', action_step.reversed), ('e', action_entity.reversed)))\n",
    "                        )\n",
    "                    )\n",
    "                )\n",
    "            ), active = True\n",
    "        ) \n",
    "\n",
    "    #  ------------ Create\n",
    "    # No subsequent create action unless there is a destroy action between them\n",
    "    \n",
    "    ifL(\n",
    "        # action a1 is create, i is a1's step and e is action entity\n",
    "        andL(\n",
    "            action_label.create('a1'), \n",
    "            step('i', path=('a1', action_step)),\n",
    "            entity('e', path=('a1', action_entity))\n",
    "            ), \n",
    "        # then either\n",
    "        orL(\n",
    "            # step j associated with entity e, which is before step i cannot be associated with create action a2\n",
    "            ifL(\n",
    "                step('j', path=(('i', before_arg2.reversed, before_arg1))), \n",
    "                notL(action_label.create('a2', path=(('j', action_step.reversed), ('e', action_entity.reversed))))\n",
    "                ), \n",
    "            # or if  \n",
    "            ifL(\n",
    "                # step j1 which is before step i is associated with create action a2\n",
    "                andL(\n",
    "                    step('j1', path=('i', before_arg2.reversed, before_arg1)), \n",
    "                    action_label.create('a2', path=(('j1', action_step.reversed), ('e', action_entity.reversed)))\n",
    "                    ), \n",
    "                # then exists step k associated with entity e, which is between step i and j1 associated with destroy action a3\n",
    "                existsL(\n",
    "                    andL(\n",
    "                        step('k', path=(('j1', before_arg1.reversed, before_arg2), ('i', before_arg2.reversed, before_arg1))), \n",
    "                        action_label.destroy('a3', path=(('k', action_step.reversed), ('e', action_entity.reversed)))\n",
    "                        )\n",
    "                    )\n",
    "                )\n",
    "            ), active = True\n",
    "        )\n",
    "    \n",
    "    #  ------------ Move\n",
    "    # No subsequent move action unless there is a create action between them\n",
    "\n",
    "    ifL(\n",
    "        # action a1 is move, i is a1's step and e is action entity\n",
    "        andL(\n",
    "            action_label.move('a1'), \n",
    "            step('i', path=('a1', action_step)),\n",
    "            entity('e', path=('a1', action_entity))\n",
    "            ), \n",
    "        # then either\n",
    "        orL(\n",
    "            # step j associated with entity e, which is before step i cannot be associated with destroy action a2\n",
    "            ifL(\n",
    "                step('j', path=(('i', before_arg2.reversed, before_arg1))), \n",
    "                notL(action_label.destroy('a2', path=(('j', action_step.reversed), ('e', action_entity.reversed))))\n",
    "            ),\n",
    "            # or if  \n",
    "            ifL(\n",
    "                # step j1 which is before step i is associated with destroy action a2\n",
    "                andL(\n",
    "                    step('j1', path=('i', before_arg2.reversed, before_arg1)), \n",
    "                    action_label.destroy('a3', path=(('j1', action_step.reversed), ('e', action_entity.reversed)))\n",
    "                    ), \n",
    "                # then exists step k associated with entity e, which is between step i and j1 associated with create action a3\n",
    "                existsL(\n",
    "                    andL(\n",
    "                        step('k', path=(('j1', before_arg1.reversed, before_arg2), ('i', before_arg2.reversed, before_arg1))), \n",
    "                        action_label.create('a4', path=(('k', action_step.reversed), ('e', action_entity.reversed)))\n",
    "                        )\n",
    "                    )\n",
    "                )\n",
    "            ), active = True\n",
    "        )\n",
    "    \n",
    "    \n",
    "    ### If the action is move, then the location from step before should be different from the current step\n",
    "    ifL(\n",
    "        # action a1 is move, i is a1's step and e is action entity\n",
    "        andL(\n",
    "            action_label.move('a1'), \n",
    "            step('i', path=('a1', action_step)),\n",
    "            entity('e', path=('a1', action_entity)),\n",
    "            entity_location_label('x', path=(('i', lstep.reversed), ('e', lentity.reversed)))\n",
    "            ), \n",
    "        andL(\n",
    "            step('j', path=('i', ebefore_arg2.reversed, ebefore_arg1)),\n",
    "            notL(entity_location_label('y', path=(('j', lstep.reversed), ('e', lentity.reversed), ('x', llocation, llocation.reversed))))\n",
    "        ),\n",
    "        active = True\n",
    "    )\n",
    "    \n",
    "    ### There can only be one location for each entity at each step\n",
    "#     ifL(\n",
    "#         andL(\n",
    "#             entity('e'),\n",
    "#             step('i')\n",
    "#         ),\n",
    "#         atMostL( \n",
    "#             entity_location_label('x', path=(('i', lstep.reversed), ('e', lentity.reversed))), 1\n",
    "#         ),\n",
    "#         active = True\n",
    "#     )\n",
    "    \n",
    "    \n",
    "    # if \n",
    "#     ifL(\n",
    "#         # action a1 is destroy, i is a1's step and e is action entity\n",
    "#         andL(\n",
    "#             action_label.destroy('a1'), \n",
    "#             step('i', path=('a1', action_step)),\n",
    "#             entity('e', path=('a1', action_entity))\n",
    "#             ), \n",
    "        \n",
    "#         # step j associated with entity e, which is before step i cannot be associated with destroy action a2\n",
    "#         andL(\n",
    "#             step('j', path=(('e', action_entity.reversed, action_step), ('i', before_arg1.reversed, before_arg2))), \n",
    "#             notL(action_label.destroy('a2', path=('j', action_step.reversed)))\n",
    "#             ), \n",
    "#         active = False\n",
    "#         ) \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "patient-cache",
   "metadata": {},
   "outputs": [],
   "source": [
    "from regr.sensor.pytorch.sensors import FunctionalSensor, JointSensor, ReaderSensor, FunctionalReaderSensor\n",
    "from regr.sensor.pytorch.learners import ModuleLearner\n",
    "from regr.sensor.pytorch.relation_sensors import CompositionCandidateSensor\n",
    "from regr.sensor.pytorch.query_sensor import DataNodeReaderSensor\n",
    "\n",
    "class JointFunctionalReaderSensor(JointSensor, FunctionalReaderSensor):\n",
    "    pass\n",
    "\n",
    "\n",
    "procedure['id'] = ReaderSensor(keyword='ProcedureID')\n",
    "context['text'] = ReaderSensor(keyword='Context')\n",
    "entities['text'] = ReaderSensor(keyword='Entities')\n",
    "locations['text'] = ReaderSensor(keyword='Locations')\n",
    "\n",
    "def make_procedure(arg1m, arg2m, arg3m, data):\n",
    "    total_procedures = len(arg1m) * len(arg2m) * len(arg3m)\n",
    "    rel_links1 = torch.ones(total_procedures, len(arg1m))\n",
    "    rel_links2 = torch.ones(total_procedures, len(arg2m))\n",
    "    rel_links3 = torch.ones(total_procedures, len(arg3m))\n",
    "#     past1 = 0\n",
    "#     past2 = 0\n",
    "#     for i in range(total_procedures):\n",
    "#         rel_links1[i, past2: past2 + len(arg2m)] = 1\n",
    "#         past2 = past2 + len(arg2m)\n",
    "#         rel_links2[i, past1: past1 + len(arg1m)] = 1\n",
    "#         past1 = past1 + len(arg1m)\n",
    "\n",
    "#     for i in range(len(arg1m)):\n",
    "#         rel_links1[0, i: (i+1)*(len(arg2m) * len(arg3m))] = 1\n",
    "        \n",
    "    \n",
    "\n",
    "    return rel_links1, rel_links2, rel_links3\n",
    "\n",
    "\n",
    "procedure[procedure_context.reversed, procedure_entities.reversed, procedure_locations.reversed] = JointFunctionalReaderSensor(context['text'], entities['text'], locations['text'], keyword=\"ProcedureID\", forward=make_procedure)\n",
    "\n",
    "def read_initials(*prev, data):\n",
    "    number = len(data)\n",
    "    rel_links = torch.ones(number, 1)\n",
    "    indexes = [i for i in range(number)]\n",
    "#     print(rel_links, data, indexes)\n",
    "    return rel_links, data, indexes\n",
    "\n",
    "entity[entity_rel, 'text', 'index'] = JointFunctionalReaderSensor(entities['text'], keyword='Entity', forward=read_initials)\n",
    "\n",
    "step[context_step, 'text', 'index'] = JointFunctionalReaderSensor(context['text'], keyword='Step', forward=read_initials)\n",
    "\n",
    "location[loc_rel, 'text', 'index'] = JointFunctionalReaderSensor(locations['text'], keyword='Location', forward=read_initials)\n",
    "\n",
    "\n",
    "\n",
    "def make_before_connection(*prev, data):\n",
    "    return data[0], data[1]\n",
    "\n",
    "before[before_arg1.reversed, before_arg2.reversed] = JointFunctionalReaderSensor(step['text'], keyword='before', forward=make_before_connection)\n",
    "\n",
    "exact_before[ebefore_arg1.reversed, ebefore_arg2.reversed] = JointFunctionalReaderSensor(step['text'], keyword='exact_before', forward=make_before_connection)\n",
    "\n",
    "\n",
    "def make_actions(r1, r2, entities, steps):\n",
    "#     print(r1, r2)\n",
    "    all_actions = len(steps) * len(entities)\n",
    "    link1 = torch.zeros(all_actions, len(steps))\n",
    "    link2 = torch.zeros(all_actions, len(entities))\n",
    "    for i in range(len(entities)):\n",
    "        link2[i*len(steps):(i+1)*len(steps),i] = 1\n",
    "\n",
    "    for j in range(all_actions):\n",
    "        link1[j, j%len(steps)] = 1\n",
    "\n",
    "#     print(link1, link2)\n",
    "#     print(link1.shape, link2.shape)\n",
    "#     print(\"steps: \", len(steps))\n",
    "#     print(\"entities: \", len(entities))\n",
    "    return link1, link2\n",
    "\n",
    "action[action_step.reversed, action_entity.reversed] = JointSensor(entity[entity_rel], step[context_step], entity['index'], step['index'], forward=make_actions)\n",
    "\n",
    "def make_same_mentions(r1, r2, entities, locations):\n",
    "    matches = []\n",
    "    for eid, ent in enumerate(entities):\n",
    "        for lid, loc in emumerate(locations):\n",
    "            if ent == loc:\n",
    "                matches.append(eid, lid)\n",
    "    link1 = torch.zeros(len(matches), len(entities))\n",
    "    link2 = torch.zeros(len(matches), len(locations))\n",
    "    \n",
    "    for mid, match in enumerate(matches):\n",
    "        link1[mid][match[0]] = 1\n",
    "        link2[mid][match[1]] = 1\n",
    "        \n",
    "    return link1, link2\n",
    "                \n",
    "\n",
    "same_mention[same_entity.reversed, same_location.reversed] = JointSensor(entity[entity_rel], location[loc_rel], entity['text'], location['text'], forward=make_same_mentions)\n",
    "\n",
    "\n",
    "def make_entity_locations(r1, r2, r3, entities, steps, locations):\n",
    "#     print(r1, r2)\n",
    "    all_actions = len(steps) * len(entities) * len(locations)\n",
    "    link2 = torch.zeros(all_actions, len(steps))\n",
    "    link1 = torch.zeros(all_actions, len(entities))\n",
    "    link3 = torch.zeros(all_actions, len(locations))\n",
    "    # for i in range(len(entities)):\n",
    "    #     link2[i*len(steps):(i+1)*len(steps),i] = 1\n",
    "\n",
    "    # for j in range(all_actions):\n",
    "    #     link1[j, j%len(steps)] = 1\n",
    "\n",
    "    for i in range(len(entities)):\n",
    "        link1[i*len(steps)*len(locations):(i+1)*len(steps)*len(locations), i] = 1\n",
    "\n",
    "    for i in range(len(entities)):\n",
    "        for j in range(len(steps)):\n",
    "            start = i*len(steps)*len(locations)\n",
    "            link2[start+(len(locations)*j): start +((j+1)*len(locations)), j] = 1\n",
    "\n",
    "    for i in range(len(entities)):\n",
    "        for j in range(len(steps)):\n",
    "            for k in range(len(locations)):\n",
    "                start = i*len(steps)*len(locations) + (j*len(locations))\n",
    "                link3[start+k] = 1\n",
    "\n",
    "#     print(link1, link2)\n",
    "#     print(link1.shape, link2.shape)\n",
    "#     print(\"steps: \", len(steps))\n",
    "#     print(\"entities: \", len(entities))\n",
    "    return link1, link2, link3\n",
    "\n",
    "\n",
    "entity_location[lentity.reversed, lstep.reversed, llocation.reversed] = JointSensor(entity[entity_rel], step[context_step], location[loc_rel], entity['index'], \n",
    "                                                                                    step['index'], location['index'], forward=make_entity_locations)\n",
    "\n",
    "def read_location_labels(*prevs, data):\n",
    "#     print(prevs[0].shape, prevs[1].shape)\n",
    "#     print(data.shape)\n",
    "    c = torch.softmax(data, dim=-1)\n",
    "    d = c.repeat(1, 1, 2)\n",
    "    d = d.view(c.shape[0], c.shape[1], c.shape[2], 2)\n",
    "    for k in range(d.shape[2]):\n",
    "        d[:, :, k, 1] = 1 - d[:, :, k, 0]\n",
    "#     print(c.shape)\n",
    "\n",
    "    d = d.view(c.shape[0] * c.shape[1] * c.shape[2], 2)\n",
    "    return d\n",
    "\n",
    "entity_location[entity_location_label] = FunctionalReaderSensor(lentity.reversed, lstep.reversed, llocation.reversed, keyword=\"LocationLabel\", forward=read_location_labels)\n",
    "\n",
    "def read_labels(*prevs, data):\n",
    "#     print(prevs[0].shape, prevs[1].shape)\n",
    "#     print(data.shape)\n",
    "    c = data.view(data.shape[0] * data.shape[1], data.shape[2])\n",
    "#     print(c.shape)\n",
    "    return c\n",
    "#     pass\n",
    "\n",
    "action[action_label] = FunctionalReaderSensor(action_step.reversed, action_entity.reversed, keyword=\"Action\", forward=read_labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "experienced-parker",
   "metadata": {},
   "outputs": [],
   "source": [
    "from regr.program import POIProgram, IMLProgram, SolverPOIProgram\n",
    "from regr.program.primaldualprogram import PrimalDualProgram\n",
    "from regr.program.model.pytorch import SolverModel\n",
    "from regr.program.metric import MacroAverageTracker, PRF1Tracker, PRF1Tracker, DatanodeCMMetric\n",
    "from regr.program.loss import NBCrossEntropyLoss, NBCrossEntropyIMLoss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "downtown-result",
   "metadata": {},
   "outputs": [],
   "source": [
    "program = SolverPOIProgram(graph, poi=(procedure, before, action, action_label, exact_before, entity_location, entity_location_label), \n",
    "                               inferTypes=['ILP', 'local/argmax'], \n",
    "                               loss=MacroAverageTracker(NBCrossEntropyLoss()), \n",
    "                               metric={'ILP':PRF1Tracker(DatanodeCMMetric()),'argmax':PRF1Tracker(DatanodeCMMetric('local/argmax'))})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "brutal-reflection",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Academic license - for non-commercial use only - expires 2023-05-20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:gurobipy.gurobipy:Academic license - for non-commercial use only - expires 2023-05-20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using license file /home/hfaghihi/gurobi.lic\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:gurobipy.gurobipy:Using license file /home/hfaghihi/gurobi.lic\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Set parameter LogFile to value logs/gurobi.log\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:gurobipy.gurobipy:Set parameter LogFile to value logs/gurobi.log\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m~/Framework/fix/DomiKnowS/examples/ModularPropara/main.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m#     lbp.test(dataset, device='auto')\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mall_updates\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mdatanode\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlbp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpopulate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"cpu\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;31m#     tdatanode = datanode.findDatanodes(select = context)[0]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;31m#     print(len(datanode.findDatanodes(select = context)))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Framework/fix/DomiKnowS/regr/program/program.py\u001b[0m in \u001b[0;36mpopulate\u001b[0;34m(self, dataset, device)\u001b[0m\n\u001b[1;32m    308\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mdevice\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    309\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 310\u001b[0;31m         \u001b[0;32myield\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpopulate_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    311\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    312\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mpopulate_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Framework/fix/DomiKnowS/regr/program/program.py\u001b[0m in \u001b[0;36mpopulate_epoch\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    315\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    316\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_item\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 317\u001b[0;31m                 \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_item\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    318\u001b[0m                 \u001b[0;32myield\u001b[0m \u001b[0mdetuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    319\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1052\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Framework/fix/DomiKnowS/regr/program/model/pytorch.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, data_item, build)\u001b[0m\n\u001b[1;32m     94\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mbuilder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mneedsBatchRootDN\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m                 \u001b[0mbuilder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maddBatchRootDN\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 96\u001b[0;31m             \u001b[0;34m*\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpopulate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbuilder\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     97\u001b[0m             \u001b[0mdatanode\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuilder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetDataNode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdatanode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuilder\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Framework/fix/DomiKnowS/regr/program/model/pytorch.py\u001b[0m in \u001b[0;36mpopulate\u001b[0;34m(self, builder, run)\u001b[0m\n\u001b[1;32m    253\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    254\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mpopulate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuilder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 255\u001b[0;31m         \u001b[0mdata_item\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minference\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbuilder\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    256\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpopulate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbuilder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    257\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Framework/fix/DomiKnowS/regr/program/model/pytorch.py\u001b[0m in \u001b[0;36minference\u001b[0;34m(self, builder)\u001b[0m\n\u001b[1;32m    248\u001b[0m                 \u001b[0;34m'argmax'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0mdatanode\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    249\u001b[0m                 \u001b[0;34m'softmax'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0mdatanode\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 250\u001b[0;31m             }[infertype]()\n\u001b[0m\u001b[1;32m    251\u001b[0m \u001b[0;31m#         print(\"Done with the inference\")\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    252\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mbuilder\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Framework/fix/DomiKnowS/regr/program/model/pytorch.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    243\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0minfertype\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minferTypes\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    244\u001b[0m             {\n\u001b[0;32m--> 245\u001b[0;31m                 \u001b[0;34m'ILP'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0mdatanode\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minferILPResults\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minference_with\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfun\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepsilon\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    246\u001b[0m                 \u001b[0;34m'local/argmax'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0mdatanode\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minferLocal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    247\u001b[0m                 \u001b[0;34m'local/softmax'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0mdatanode\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minferLocal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Framework/fix/DomiKnowS/regr/graph/dataNode.py\u001b[0m in \u001b[0;36minferILPResults\u001b[0;34m(self, fun, epsilon, minimizeObjective, ignorePinLCs, *_conceptsRelations)\u001b[0m\n\u001b[1;32m   1116\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1117\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minferLocal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1118\u001b[0;31m         \u001b[0mmyilpOntSolver\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcalculateILPSelection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mconceptsRelations\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfun\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfun\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepsilon\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mepsilon\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mminimizeObjective\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mminimizeObjective\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignorePinLCs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mignorePinLCs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1119\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1120\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mverifySelection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0m_conceptsRelations\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Framework/fix/DomiKnowS/regr/solver/gurobiILPOntSolver.py\u001b[0m in \u001b[0;36mcalculateILPSelection\u001b[0;34m(self, dn, fun, epsilon, minimizeObjective, ignorePinLCs, *conceptsRelations)\u001b[0m\n\u001b[1;32m   1221\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1222\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mpUsed\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1223\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maddLogicalConstrains\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmP\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlcs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlckey\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# <--- LC constraints\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1224\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1225\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreuse_model\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Framework/fix/DomiKnowS/regr/solver/gurobiILPOntSolver.py\u001b[0m in \u001b[0;36maddLogicalConstrains\u001b[0;34m(self, m, dn, lcs, p, key)\u001b[0m\n\u001b[1;32m    607\u001b[0m                 \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    608\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 609\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__constructLogicalConstrains\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmyIlpBooleanProcessor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0mlcVariablesDns\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheadLC\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    610\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    611\u001b[0m             \u001b[0mm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Framework/fix/DomiKnowS/regr/solver/gurobiILPOntSolver.py\u001b[0m in \u001b[0;36m__constructLogicalConstrains\u001b[0;34m(self, lc, booleanProcesor, m, dn, p, key, lcVariablesDns, headLC, loss, sample, vNo)\u001b[0m\n\u001b[1;32m   1048\u001b[0m                     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1049\u001b[0m                         vDns = self.__constructLogicalConstrains(e, booleanProcesor, m, dn, p, key = key, \n\u001b[0;32m-> 1050\u001b[0;31m                                                                  lcVariablesDns = lcVariablesDns, headLC = False, loss = loss, sample = sample, vNo=vNo)\n\u001b[0m\u001b[1;32m   1051\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1052\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mvDns\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Framework/fix/DomiKnowS/regr/solver/gurobiILPOntSolver.py\u001b[0m in \u001b[0;36m__constructLogicalConstrains\u001b[0;34m(self, lc, booleanProcesor, m, dn, p, key, lcVariablesDns, headLC, loss, sample, vNo)\u001b[0m\n\u001b[1;32m    936\u001b[0m                                 \u001b[0;31m# --- Assume Intersection - TODO: in future use lo if defined to determine if different operation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    937\u001b[0m                                 \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdnsListForPaths\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 938\u001b[0;31m                                     \u001b[0mse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdnsListForPaths\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mitem\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpathsCount\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    939\u001b[0m                                     \u001b[0mdnsListR\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreduce\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mintersection\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    940\u001b[0m                                     \u001b[0mdnsList\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdnsListR\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Framework/fix/DomiKnowS/regr/solver/gurobiILPOntSolver.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    936\u001b[0m                                 \u001b[0;31m# --- Assume Intersection - TODO: in future use lo if defined to determine if different operation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    937\u001b[0m                                 \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdnsListForPaths\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 938\u001b[0;31m                                     \u001b[0mse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdnsListForPaths\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mitem\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpathsCount\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    939\u001b[0m                                     \u001b[0mdnsListR\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreduce\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mintersection\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    940\u001b[0m                                     \u001b[0mdnsList\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdnsListR\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "lbp = program\n",
    "\n",
    "dataset = ProparaReader(file=\"data/train.json\")  # Adding the info on the reader\n",
    "\n",
    "\n",
    "#     lbp.test(dataset, device='auto')\n",
    "all_updates = []\n",
    "for datanode in lbp.populate(dataset, device=\"cpu\"):\n",
    "#     tdatanode = datanode.findDatanodes(select = context)[0]\n",
    "#     print(len(datanode.findDatanodes(select = context)))\n",
    "#     print(tdatanode.getChildDataNodes(conceptName=step))\n",
    "    datanode.inferILPResults(action, fun=None)\n",
    "    final_output = {\n",
    "        \"id\": datanode.getAttribute(\"id\"),\n",
    "        \"steps\": [],\n",
    "        \"actions\": [],\n",
    "        \"steps_before\": [],\n",
    "        \"actions_before\": [],\n",
    "    }\n",
    "\n",
    "    entities_instances = datanode.findDatanodes(select=entity)\n",
    "#     print(len(entities))\n",
    "    steps_instances = datanode.findDatanodes(select=step)\n",
    "    actions = datanode.findDatanodes(select=action)\n",
    "#         print('a')\n",
    "\n",
    "    for step_instance in steps_instances:\n",
    "        a = step_instance.getAttribute('index')\n",
    "        # rel = step.getRelationLinks(relationName=before)\n",
    "        # print(rel)\n",
    "    # print(len(steps_instances), \"\\n\")\n",
    "    for action_info in datanode.findDatanodes(select=action):\n",
    "        c = action_info.getAttribute(action_label, \"ILP\")\n",
    "        final_output[\"actions\"].append(c)\n",
    "        c1 = action_info.getAttribute(action_label)\n",
    "        final_output[\"actions_before\"].append(c1)\n",
    "\n",
    "    final_output['actions'] = torch.stack(final_output['actions'])\n",
    "    final_output['actions'] = final_output['actions'].view(len(entities_instances), len(steps_instances), 4)\n",
    "\n",
    "    final_output['actions_before'] = torch.stack(final_output['actions_before'])\n",
    "    final_output['actions_before'] = final_output['actions_before'].view(len(entities_instances), len(steps_instances), 4)\n",
    "\n",
    "    all_updates.append(final_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "completed-customer",
   "metadata": {},
   "outputs": [],
   "source": [
    "count = 0\n",
    "for item in all_updates:\n",
    "    if not torch.equal(item['actions_before'].argmax(dim=-1), item['actions'].argmax(dim=-1)):\n",
    "        print(item['actions_before'].argmax(dim=-1))\n",
    "        print(item['actions'].argmax(dim=-1))\n",
    "        print(count)\n",
    "        print(\"----\" * 5, '\\n')\n",
    "    count += 1\n",
    "# all_updates[number]['actions_before'].argmax(dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hearing-breathing",
   "metadata": {},
   "outputs": [],
   "source": [
    "number = 4\n",
    "all_updates[number]['actions_before'].argmax(dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "accompanied-daisy",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_updates[number]['actions'].argmax(dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "sticky-spoke",
   "metadata": {},
   "outputs": [],
   "source": [
    "idx2state = {0: 'NoChange', 1: 'Move', 2: 'Create', 3: 'Destroy'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "removed-first",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(\n",
    "        probability, data_item\n",
    "    ):\n",
    "        all_predictions = {\"actions\": [], \"locations\": []}\n",
    "\n",
    "        specific_correct = {\n",
    "            \"tags\": 0,\n",
    "        }\n",
    "        specific_total = {\n",
    "            \"tags\": 0,\n",
    "        }\n",
    "\n",
    "        keys = [\"tags\", 'location']\n",
    "\n",
    "        for key in keys:\n",
    "            if key == \"tags\":\n",
    "                preds = torch.tensor(probability[key])\n",
    "\n",
    "                idx2state = {0: 'O_C', 1: 'M', 2: 'C', 3: 'D'}\n",
    "                original_idx2state = {0: 'O_C', 1: 'O_D', 2: 'E', 3: 'M', 4: 'C', 5: 'D'}\n",
    "\n",
    "                for eid in range(preds.shape[0]):\n",
    "                    all_predictions['actions'].append([])\n",
    "                    for sid in range(preds.shape[1]):\n",
    "                        all_predictions['actions'][-1].append(idx2state[preds[eid][sid].item()])\n",
    "\n",
    "            elif key == \"location\":\n",
    "                preds = probability[key]\n",
    "\n",
    "                # if self.config.unknown_candidate:\n",
    "                #     if len(grounding[\"candidates\"]):\n",
    "                #         preds = probability[key].squeeze(-1)\n",
    "                #         preds = torch.softmax(preds, dim=-1)\n",
    "                #         preds_score, preds_index = torch.max(preds, dim=-1)\n",
    "                # else:\n",
    "                #     if len(grounding[\"candidates\"]):\n",
    "                #         preds = probability[key].squeeze(-1)\n",
    "                #         preds = torch.sigmoid(preds)\n",
    "                #         preds_score, preds_index = torch.max(preds, dim=-1)\n",
    "                #         for pid, pvalue in enumerate(preds_score):\n",
    "                #             if pvalue <= 0.5:\n",
    "                #                 preds_index[pid] = -1\n",
    "                #     else:\n",
    "                #         preds_index = []\n",
    "                #         for pvalue in target:\n",
    "                #             preds_index.append(-1)\n",
    "                #         preds_index = torch.tensor(preds_index, device=self.device)\n",
    "\n",
    "\n",
    "                preds = torch.softmax(torch.tensor(preds), dim=-1)\n",
    "                preds_score, preds_index = torch.max(preds, dim=-1)\n",
    "                preds_eval = preds_index.view(preds_index.shape[0]*preds_index.shape[1])\n",
    "\n",
    "                for eid in range(preds.shape[0]):\n",
    "                    all_predictions['locations'].append([])\n",
    "                    for sid in range(preds.shape[1]):\n",
    "                        if preds_index[eid][sid] == len(data_item['location_cand']):\n",
    "                            all_predictions['locations'][-1].append(\"?\")\n",
    "                        else:\n",
    "                            all_predictions['locations'][-1].append(data_item[\"location_cand\"][preds_index[eid][sid]])\n",
    "\n",
    "        return all_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "small-shanghai",
   "metadata": {},
   "outputs": [],
   "source": [
    "for item in data:\n",
    "    up = [it for it in all_updates if it['id'] == item['para_id']][0]\n",
    "    assert len(item['net_results']['tags']) == len(up['actions'].argmax(dim=-1).tolist())\n",
    "    assert len(item['net_results']['tags'][0]) == len(up['actions'].argmax(dim=-1).tolist()[0])\n",
    "    item['net_results']['tags'] = up['actions'].argmax(dim=-1).tolist()\n",
    "#     print(item['net_results']['tags'], \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "olive-wings",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "union-colorado",
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_table = []\n",
    "prediction_table = []\n",
    "for item in data:\n",
    "#     print(item['para_id'])\n",
    "    all_predictions = evaluate(item['net_results'], item)\n",
    "    step_predictions = []\n",
    "    try:\n",
    "        for sid in range(len(item['sentence_texts']) + 1):\n",
    "            for evalid, entity_text in enumerate(item[\"participants\"]):\n",
    "                if sid != 0:\n",
    "                    step_predictions.append(\n",
    "                        {\n",
    "                            \"para_id\": item[\"para_id\"],\n",
    "                            \"step\": sid,\n",
    "                            \"entity\": entity_text,\n",
    "                            \"action\": all_predictions[\"actions\"][evalid][sid-1],\n",
    "                            \"location\": all_predictions[\"locations\"][evalid][sid],\n",
    "                        }\n",
    "                    )\n",
    "                else:\n",
    "                    step_predictions.append(\n",
    "                        {\n",
    "                            \"para_id\": item[\"para_id\"],\n",
    "                            \"step\": sid,\n",
    "                            \"entity\": entity_text,\n",
    "                            \"location\": all_predictions[\"locations\"][evalid][sid],\n",
    "                        }\n",
    "                    )\n",
    "\n",
    "        prediction_table.extend(step_predictions)\n",
    "    except:\n",
    "        print(sid, len(all_predictions['actions'][0]), evalid, len(item['sentence_texts']))\n",
    "        raise\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "vertical-difficulty",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "csv_file = (\n",
    "    \"./data/random.tsv\"\n",
    ")\n",
    "with open(csv_file, \"w\", newline=\"\") as f_output:\n",
    "    tsv_output = csv.writer(f_output, delimiter=\"\\t\")\n",
    "    for sample_pred in prediction_table:\n",
    "        if \"action\" in sample_pred:\n",
    "            dd = [\n",
    "                str(sample_pred[\"para_id\"]),\n",
    "                sample_pred[\"step\"],\n",
    "                sample_pred[\"entity\"],\n",
    "                sample_pred[\"action\"],\n",
    "                \"?\",\n",
    "                sample_pred[\"location\"],\n",
    "            ]\n",
    "        else:\n",
    "            dd = [\n",
    "                str(sample_pred[\"para_id\"]),\n",
    "                sample_pred[\"step\"],\n",
    "                sample_pred[\"entity\"],\n",
    "                \"-\",\n",
    "                \"-\",\n",
    "                sample_pred[\"location\"],\n",
    "            ]\n",
    "        tsv_output.writerow(dd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "restricted-structure",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tools import get_koala_fix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "fifty-frederick",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fix_tsv():\n",
    "    preds = []\n",
    "    with open(\n",
    "        \"./data/random.tsv\"\n",
    "    ) as csv_file:\n",
    "        # with open('test.tsv') as csv_file:\n",
    "        csv_reader = csv.reader(csv_file, delimiter=\"\\t\")\n",
    "        for row in csv_reader:\n",
    "            preds.append(row)\n",
    "\n",
    "    fixed_pred = get_koala_fix(preds)\n",
    "\n",
    "    csv_file_logical = \"./data/logical.tsv\"\n",
    "    with open(csv_file_logical, \"w\", newline=\"\") as f_output:\n",
    "        tsv_output = csv.writer(f_output, delimiter=\"\\t\")\n",
    "        for item in fixed_pred:\n",
    "            item = [\n",
    "                item[\"para_id\"],\n",
    "                item[\"step\"],\n",
    "                item[\"entity\"],\n",
    "                item[\"action\"],\n",
    "                item[\"before\"],\n",
    "                item[\"after\"],\n",
    "            ]\n",
    "            tsv_output.writerow(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "experimental-creation",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['37', '0', 'plant; animal', '-', '-', '?']\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "preds = []\n",
    "with open(\n",
    "    \"./data/random.tsv\"\n",
    ") as csv_file:\n",
    "    # with open('test.tsv') as csv_file:\n",
    "    csv_reader = csv.reader(csv_file, delimiter=\"\\t\")\n",
    "    for row in csv_reader:\n",
    "        preds.append(row)\n",
    "        \n",
    "print(preds[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "undefined-engine",
   "metadata": {},
   "outputs": [],
   "source": [
    "fix_tsv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "tamil-lithuania",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
